\documentclass{article}
% \usepackage{corl_2022} % Use this for the initial submission.
\usepackage[final]{corl_2022} % Uncomment for the camera-ready ``final'' version.
\usepackage{times} % assumes new font selection scheme installed
\usepackage{brian}
\graphicspath{{../images/}}
\usepackage[export]{adjustbox}
% \usepackage{biblatex}

% Bibliography
% \usepackage[style=ieee,doi=false,isbn=false,url=false,eprint=false]{biblatex}
% \AtEveryBibitem{\clearlist{language}}  % remove language field from bib entries
% \addbibresource{references.bib}
% \renewcommand*{\bibfont}{\small}


\title{JDMD: Extended Dynamic Mode Decomposition with Jacobian Residual Penalization
for Learning Bilinear, Control-affine Koopman Models}

\author{
  Brian E. Jackson \\
  Robotics Institute \\
  Carnegie Mellon University\\
  \texttt{brianjackson@cmu.edu} \\
  \and
  Jeong Hun Lee \\
  Robotics Institute\\
  Carnegie Mellon University\\
  \texttt{jeonghunlee@cmu.edu} \\
  \and
  Kevin Tracy \\
  Robotics Institute\\
  Carnegie Mellon University\\
  \texttt{ktracy@cmu.edu} \\
  \and
  Zachary Manchester \\
  Robotics Institute\\
  Carnegie Mellon University\\
  \texttt{zacm@cmu.edu} \\
}

\begin{document}
\maketitle

\todo{Add abstract}

\section{Introduction}

    Controlling complex, underactuated, and highly nonlinear autonomous systems remains an
    active area of research in robotics, despite decades of previous work exploring
    effective algorithms and the development of substantial theoretical analysis. Classical
    approaches typically rely on local linear approximations of the nonlinear system, which
    are then used in any of a multitude of linear control techniques, such as PID, pole
    placement, Bode analysis, H-infinity, LQR, or linear MPC.  These approaches only work
    well if the states of the system always remain close to the equilibrium point or
    reference trajectory about which the system was linearized. The region for which these
    linearizations remain valid can be extremely small for highly nonlinear systems.
    Alternatively, model-based methods for nonlinear control based on optimal control have
    shown great success, as long as the model is well known and an accurate estimate of the
    global state can be provided. These model-based techniques leverage centuries of insight
    into dynamical systems and have demonstrated incredible performance on extremely
    complicated autonomous systems 
    \cite{farshidian_efficient_2017,Kuindersma2014,Bjelonic2021,Subosits2019} .  On the
    other hand, data-driven techniques such as reinforcement learning have received
    tremendous attention over the last decade and have begun to demonstrate impressive
    performance and robustness for complicated robotic systems in unstructured environments
    \cite{Karnchanachari2020,Hoeller2020,Li2021}. While these approaches are attractive
    since they require little to no previous knowledge about the system, they often require
    large amounts of data and fail to generalize outside of the domain or task on which they
    were ``trained.''
    
    In this work we propose a novel method that combines the benefits of model-based and
    data-driven methods, based on recent work  applying Koopman Operator Theory to
    controlled dynamical systems 
    \cite{Meduri2022,Bruder2021,Korda2018,Folkestad2020,Suh2020}.
    % \cite{Meduri2022,Bruder2021, Korda2021}
    % \cite{Meduri2022, Folkestad2021, Bruder2021, Korda2018, Folkestad2020a, Folkestad2020b}.  
    By leveraging data collected from an
    unknown dynamical system along with derivative information from an approximate
    analytical model, we can efficiently learn a bilinear representation of the system
    dynamics that performs well when used in traditional model-based control techniques such
    as linear MPC. The result is nonlinear dynamical system that is expressive enough to
    capture the full nonlinear dynamics across a broad range of the state space, but has a
    well-defined structure that is very amenable to specialized optimization-based
    controllers. By leveraging information from an analytical model, we can dramatically
    reduce the number of samples required to learn a good approximation of the true
    nonlinear dynamics. We also show the effiectiveness of linear MPC on these systems, 
    when the learned bilinear system is linearized and projected back into the original 
    state space. The result is a fast, robust, and sample-efficient pipeline for quickly 
    learning a model that beats previous Koopman-based linear MPC approaches as well as 
    purely model-based linear MPC contollers that do not leverage data collected from the 
    actual system. To efficiently learn these bilinear representations, we also propose 
    a numerical technique that allows for large systems to be trained while limiting the 
    peak memory required to solve the least-squares problem.

    In summary, our contributions are:
    * A novel extension to extended dynamic model decomposition (eDMD) that incorporates 
      gradient information from an approximate analytic model
    * A simple linear MPC control technique for learned bilinear control systems that is 
      computationally efficient online, which, when combined with the proposed extension 
      to eDMD, requires extremely little training data to get a good control policy
    * A novel numerical method for recursively solving the least-squares problems that arise 
      when learning bilinear dynamical systems using eDMD
    
    The paper is organized as follows: in Section \ref{sec:Preliminaries/Background} we 
    give some background on the appliciation of Koopman operator theory to controlled 
    dynamical systems and review some related works. Section \ref{sec:methodology} describes
    the proposed algorithm for combining data-driven and model-based approaches, along with 
    the numerical method for solving the resulting large and sparse linear least-squares 
    problems. Section \ref{sec:results} provides extensive numerical analysis of the 
    proposed algorithm, applied to a simulated cartpole and planar quadrotor model, both 
    subject to significant model mistmach. In Section \ref{sec:limitations} we discuss the 
    limitations of the method, and finish with some concluding thoughts in Section 
    \ref{sec:conclusion}.

\section{Background and Related Work} \label{sec:Preliminaries/Background}

  \subsection{The Koopman Operator}
  The theoretical underpinnings of the Koopman operator and its application to dynamical
  systems has been extensively studied, especially within the last decade 
  \cite{Fasel2021,Proctor2018,Bruder2021,Williams2015}. Rather than describe the theory in
  detail, we highlight the key concepts employed by the current work, and defer the
  motivated reader to the existing literature on Koopman theory.

  We start by assuming we have some discrete approximation a of controlled nonlinear,
  time-dynamical system whose underlying continuous dynamics are Lipschitz continuous:
  \begin{equation} \label{eq:discrete_dynamics} x_{k+1} = f(x_k, u_k) \end{equation} where
  $x \in \mathcal{X} \subseteq \R^{N_x}$ is the state vector and $u_k \in \R^{N_u}$ is the
  control vector.  This discrete approximation can be obtained for any continuous-time, 
  smooth dynamical system in many ways, including implicit and explicit Runge-Kutta methods,
  or by solving the Discrete Euler-Lagrange equations
  \cite{Brudigam2021a,Brudigam2021,Howell2022}.

  The key idea behind the Koopman operator is that the nonlinear finite-dimensional discrete
  dynamics \eqref{eq:discrete_dynamics} can be represented by an infinite-dimensional
  \textit{bilinear} system:
  \begin{equation} \label{eq:bilinear_dynamics}
      y_{k+1} = A y_k + B u_k + \sum_{i=1}^m u_{k,i} C_i y_k = g(y,u)
  \end{equation}
  where $y = \phi(x)$ is a nonlinear mapping from the finite-dimensional state space
  $\mathcal{X}$ to the (possibly) infinite-dimensional Hilbert space of \textit{observables}
  $y \in \mathcal{Y}$. We also assume the inverse map is approximately linear: $x = G y$. In
  practice, we approximate \eqref{eq:discrete_dynamics} by choosing $\phi$ to be some
  arbitrary finite set of nonlinear functions of the state variables, which in general
  include the states themselves such that the linear mapping $G \in \R^{N_x \times N_y}$ is
  exact.  Intuitively, $\phi$ ``lifts'' our states into a higher dimensional space where the
  dynamics are approximately (bi)linear, effectively trading dimensionality for
  (bi)linearity. This idea should be both unsurprising and familiar to most roboticsts,
  since similar techniques have already been employed in other forms, such as
  maximal-coordinate representations of rigid body dynamics
  \cite{baraff_linear-time_1996-1,Brudigam2021a,Howell2022}, the
  ``kernel trick'' for state-vector machines \cite{Hofmann2006}, or the observation that
  solving large, sparse nonlinear optimization problems is often more effective than solving
  small, tightly-coupled dense problems \todo{add citations from the trajectory optimization
  literature}.

  The lifted bilinear system \eqref{eq:bilinear_dynamics} can be easily learned from samples
  of the system dynamics $(x_i^+,x_i,u_i)$ (where $x^+$ is the state at the next time step)
  using extended Dynamic Mode Decomposition (eDMD) \cite{Williams2015}, which is just the
  application of linear least squares (LLS) to the lifted states. Details of this method
  will be covered in the next section where we introduce our adaptation of eDMD and present
  an effective numerical technique for solving the resulting LLS problems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EDMD with Jacobian Residual-Penalization} \label{sec:methodology}
  Existing Koopman-based approaches to learning dynamical systems only rely on samples of
  the unknown dynamics. Here we present a novel method for incorporating prior knowledge
  about the dynamics by adding derivative information of an approximate model into the data
  set to be learned via eDMD.

  Given $P$ samples of the dynamics $(x_i^+, x_i, u_i)$, and an approximate discrete
  dynamics model 
  \begin{equation}
      x^+ = \tilde{f}(x,u)
  \end{equation}
  we can evaluate the Jacobians of our approximate model $\tilde{f}$ at each of the sample
  points: $\tilde{A}_i = \pdv{\tilde{f}}{x}, \tilde{B}_i = \pdv{\tilde{f}}{u}$. After
  choosing a nonlinear mapping $\phi : \R^{N_x} \mapsto \R^{N_y}$ our goal is to find a
  bilinear dynamics model \eqref{eq:bilinear_dynamics} that matches the Jacobians of our
  approximate model, while also matching our dynamics samples. If we define $\hat{A}_j \in
  \R^{N_x \times N_x}$ and $\hat{B}_j \in \R^{N_x \times N_u}$ to be the Jacobians of our
  bilinear dynamics model, projected back into the original state space (a formal definition 
  of these terms will be provided in a few paragraphs), our objective is to
  find the matrices $A \in \R^{N_y \times N_y},B \in \R^{N_y \times N_u}$, and $C_{1:m} \in
  \R^{N_u} \times \R^{N_y \times N_y}$ that minimize the following objective:

  \begin{equation} \label{eq:lls_objective}
      (1- \alpha) \sum_{j=1}^P \norm{\hat{y}_j - y_j^+}_2^2 + 
          \alpha  \sum_{j=1}^P \norm{\hat{A}_j - \tilde{A}_j}_2^2 + 
                               \norm{\hat{B}_j - \tilde{B}_j}_2^2 
  \end{equation}
  where $\hat{y}_j^+ = g\left(\phi(x_j), u_j\right)$ is the output of our bilinear dynamics
  model, and $y_j^+ = \phi(y_j^+)$ is the actual lifted state (i.e. observables) at the next
  time step. Note that $\hat{y}_j$, $\hat{A}_j$, and $\hat{B}_j$ are all implicitly
  functions of model parameters $A$, $B$, and $C_{1:m}$ we're trying to learn.

  While not immediately apparent, we can minimize \eqref{eq:lls_objective} using linear
  least-squares, using techniques similar to those used previously in the literature
  \cite{Folkestad2021}.

  To start, we combine all the data we're trying to learn in a single matrix:
  \begin{equation}
      E = \begin{bmatrix} A & B & C_1 & \dots & C_m \end{bmatrix} \in \R^{N_y \times N_z} \\
  \end{equation}
  where $N_z = N_y + N_u + N_y \cdot N_u$.  We now rewrite the terms in
  \eqref{eq:lls_objective} in terms of $E$. By defining the vector 
  \begin{equation}
      z = \begin{bmatrix} y^T & u^T & u_1 y^T & \dots & u_m y^T \end{bmatrix} \in \R^{N_z} 
  \end{equation}
  we can write down 
  the output of our bilinear dynamics \eqref{eq:bilinear_dynamics} as 
  \begin{equation} \label{eq:bilinear_dynamics_z}
      \hat{y}^+ = E z.
  \end{equation}
  The previously-mentioned projected Jacobians of our bilinear model, $\hat{A}$ and
  $\hat{B}$, are simply the Jacobians of the bilinear dynamics in terms of the original
  state. We obtain these dynamics by ``lifting`` the state via $\phi$ and then projecting
  back onto the original states using $G$:
  \begin{equation} \label{eq:projected_dynamics}
      x^+ = G \left( A \phi(x) + B u + \sum_{i=1}^m u_i C_i \phi(x) \right)  = \hat{f}(x,u) 
  \end{equation}
  Differentiating these dynamics gives us our projected Jacobians:
  \begin{subequations} \label{eq:projected_jacobians}
  \begin{align}
      \hat{A}_j &= G \pdv{\hat{f}}{x}\left(x_j,u_j\right) 
                = G \left(A + \sum_{i=1}^m u_{j,i} C_i \right) \Phi(x_j)
              %   = G A_j^x \phi(x_j)
                = G E \bar{A}(x_j,u_j) = G E \bar{A}_j \\
      \hat{B}_i &= G \pdv{\hat{f}}{u}\left(x_j,u_j\right) 
                = G \Big(B + \begin{bmatrix} C_1 x_j & \dots & C_m x_j \end{bmatrix} \Big)
              %   = G B_j^u
                = G E \bar{B}(x_j,u_j) = G E \bar{B}_j
  \end{align}
  \end{subequations}
  where $\Phi(x) = \pdv*{\phi}{x}$ is the Jacobian of the nonlinear map $\phi$, and
  \begin{equation}
      \bar{A}(x,u) =  \begin{bmatrix} 
          I \\ 0 \\ u_1 I \\ u_2 I \\ \vdots \\ u_m I 
      \end{bmatrix} \in \R^{N_z \times N_x}, \quad
      \bar{B}(x,u) = \begin{bmatrix} 
          0 \\ 
          I \\ 
          [x \; 0 \; ... \; 0] \\
          [0 \; x \; ... \; 0] \\
          \vdots \\
          [0 \; 0 \; ... \; x] \\
      \end{bmatrix} \in \R^{N_z \times N_u}.
  \end{equation}
  Note we define $\bar{A}_j = \bar{A}(x_j,u_j)$, $\bar{B}_j = \bar{B}(x_j,u_j)$ to lighten 
  the notation, but want to emphasize that these terms are all purely functions of the input
  data.

  Substituting \eqref{eq:bilinear_dynamics_z} and \eqref{eq:projected_jacobians} into
  \eqref{eq:lls_objective}, we can rewrite our LLS problem as:
  \begin{align}
      \underset{E}{\text{minimize}} \;\; 
          \sum_{j=0}^P
          (1-\alpha) \norm{E z_j - y_j^+}_2^2 + 
            \alpha  \norm{G E \bar{A}_j - \tilde{A}_j}_2^2 + 
            \alpha  \norm{G E \bar{B}_j - \tilde{B}_j}_2^2 
  \end{align}
  which is equivalent to
  \begin{align} \label{opt:lls_matrix}
      \underset{E}{\text{minimize}} \;\; 
          (1-\alpha) \norm{E \mathbf{Z_{1:P}} - \mathbf{Y^+_{1:P}} }_2^2 + 
            \alpha  \norm{G E \mathbf{\bar{A}_{1:P}} - \mathbf{\tilde{A}_{1:P}}}_2^2 + 
            \alpha  \norm{G E \mathbf{\bar{B}_{1:P}} - \mathbf{\tilde{B}_{1:P}}}_2^2
  \end{align}
  where $\mathbf{Z_{1:P}} \in \R^{N_z \times P} = [z_1 \; z_2 \; ... \; z_P]$ horizontally
  concatenates all of the samples (equivalent definition for 
  $\mathbf{Y^+_{1:P}} \in \R^{N_y \times P}$, 
  $\mathbf{\bar{A}_{1:P}} \in \R^{N_z \times N_x \cdot P}$, 
  $\mathbf{\tilde{A}_{1:P}} \in \R^{N_z \times N_x \cdot P}$,
  $\mathbf{\bar{B}_{1:P}} \in \R^{N_z \times N_u \cdot P}$, and 
  $\mathbf{\tilde{B}_{1:P}} \in \R^{N_z \times N_u \cdot P}$ ).

  We can rewrite \eqref{opt:lls_matrix} in standard form using the ``vec trick''
  \begin{equation} \label{eq:vectrick}
      \text{vec}(A X B) = (B^T \otimes A) \text{vec}(X)
  \end{equation}
  where $\text{vec}(A)$ stacks the columns of $A$ into a single vector.

  Setting $E$ in \eqref{opt:lls_matrix} equal to $X$ in \eqref{eq:vectrick}, we get
  \begin{align} \label{opt:lls_matrix}
      \underset{E}{\text{minimize}} \;\;  
      \norm{
          \begin{bmatrix}
              (1-\alpha)\cdot(\mathbf{Z_{1:P}})^T \otimes I_{N_y} \\
              \alpha\cdot(\mathbf{\bar{A}_{1:P}})^T \otimes G \\
              \alpha\cdot(\mathbf{\bar{G}_{1:P}})^T \otimes G \\
          \end{bmatrix}
          \text{vec}(E)
          +
          \begin{bmatrix}
              (1-\alpha)\cdot\text{vec}(\mathbf{Y^+_{1:P}}) \\
              \alpha\cdot\text{vec}(\mathbf{\tilde{A}_{1:P}}) \\
              \alpha\cdot\text{vec}(\mathbf{\tilde{G}_{1:P}})
          \end{bmatrix}
      }_2^2
  \end{align}
  such that the matrix of cofficients has $(N_y + N_x^2 + N_x \cdot N_u) \cdot P$ rows and 
  $N_y \cdot N_z$ columns. We obtain the data for our bilinear model 
  \eqref{eq:bilinear_dynamics} by solving this large, sparse linear least-squares 
  problem.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results} \label{sec:results}
All continuous dynamics were discretized with an explicit fourth-order Runge Kutta 
integrator.

\subsection{Training}

All models were trained by simulating the ``real'' system with an arbitrary controller to 
collect data in the region of the state space relevant to the task. A set of fixed-length 
trajectories were collected, each at a sample rate of 20 Hz. The bilinear eDMD model was
trained using the same approach in \cite{Folkestad2021}. For the proposed jDMD method, the
Jacobians of the nominal model were calculated at each of the sample points and the bilinear
model was learned using the approach outlined in Section \ref{sec:methodology}.

\subsection{Cartpole}
As a simple benchmark example, we use the canonical cartpole system. In lieu of an actual 
hardware experiment (left for future work), we specify two separate analytical models used 
in simulation: the \textit{nominal} model is a simple cartpole system without friction or 
damping, whereas the \textit{simulated} model, used exclusively for simulating the system
in place of a real hardware platform, includes viscous damping on both degrees of freedom,
a tanh Coloumb friction model for the cart, and a deadband on the control signal. 
Additionally, we altered the mass of the cart and pole from the nominal model by 20\% and 
25\%, respectively. See the provided code for the actual values and models used in the 
experiments.

We split the analysis of this system into two separate tasks: stabilization about the upward
unstable equilibrium from perturbed initial condtions, and the swing-up task where the 
system must successfully stabilize after starting from the downward equilibrium.

\subsubsection{Stabilization}

For stabilizing the system about the upward unstable equilibrium, we used an LQR controller
designed using to nominal model to collect trajectories on the simulated system.  To learn
the bilinear models, we used the following nonlinear mapping: $\phi(x) = [\; 1,\; x,\;
\sin(x),\; \cos(x),\; \sin(2x)\; ] \in \R^{17}$.  After learning both models, an MPC
controller was designed using the nominal, eDMD, and jDMD models. For the learned bilinear
models, MPC controllers were designed using the ``lifted'' Jacobians of the bilinear 
dynamics and the ``projected'' Jacobians \eqref{eq:projected_jacobians}. The MPC controllers
linearized the dynamics about the equilibrium of $x = [\;0,\; \pi,\; 0,\;, 0\;]$ and solved
the resulting equality-constrained quadratic program using Riccati recursion and a horizon
of 41 time steps (2 seconds). 

To analyze sample efficiency of the algorithms, we trained the bilinear models with an 
increasing number of samples. Each controller was tested using 100 different initial
conditions sampled from a uniform distribution of initial conditions centered about the 
upward equilibirum. The average L2 error of the state after 4 seconds and the upward 
equilibrium was recorded for each controller. The minimum number of training trajectories to
get performance better than the nominal MPC controller is reported in Figure 
\ref{fig:cartpole_lqr_samples}. As shown, the proposed approach is more sample efficient 
even for this relatively simple task than traditional eDMD. It also shows the benefit of 
applying the control in the original state space by projecting the linearization back to the
original states. To our knowledge, no previous works on applying DMD or Koopman operator 
theory to controlled systems have used this technique of projecting back into the original 
state space, although the benefits are immediately apparent: with just a few training 
trajectories (3 for jDMD and 7 for eDMD) we can learn a model that improves upon our nominal
LQR controller policy.

\subsection{Efficient Recursive Least Squares}
In its canonical formulation, a linear least squares problem can be represented as the
following unconstrained optimization problem:
\begin{align}
    \min_x \|Fx - d\|_2^2.
\end{align}
The solution to this problem is found by solving for the $x$ in which the gradient of the
objective function with respect to $x$ is zero, also known as the normal equations: 
\begin{align}\label{eq:normal_eq}
    (F^TF)x =F^Td,
\end{align}
For small to medium sized problems, this problem is most often solved with either a Cholesky
or a QR decomposition.  Unfortunately, for very large problems where storage size and
numerical conditioning become a concern, forming and factorizing the required matrices can
be intractable.

To deal with large problems like the one proposed in \eqref{opt:lls_matrix}, a recursive
method is used that processes rows of $F$ and $d$ sequentially in batches, avoiding the need
for forming or factorizing the whole matrix. To do this, the rows of $F$ and $d$ will be
divided up into batches in the following manner:
\begin{equation}
    \begin{aligned}
        F = \begin{bmatrix} F_1 \\ F_2 \\ \vdots \\ F_N \end{bmatrix}
    \end{aligned},
    \quad 
    \begin{aligned}
        d = \begin{bmatrix} d_1 \\ d_2 \\ \vdots \\ d_N \end{bmatrix}.
    \end{aligned}
\end{equation}
The matrix $F^TF$ from \eqref{eq:normal_eq} can then be represented as the following sum:
\begin{align}
    F^TF = F_1^TF_1 + F_2^TF_2 + \ldots + F_N^TF,
\end{align}
with the right-hand side vector in \eqref{eq:normal_eq} expressed in a similar fashion:
\begin{align}
    F^Td = F_1^Td_1 + F_2^Td_2 + \ldots + F_N^Td_N.
\end{align}
The main idea of this recursive method is to assemble an upper triangular factorization of
$A^TA$ by forming the factor of $A_1^TA_1$, and updating this factor with each additional
batch. To do this, first a matrix "square-root" $U$ is defined as:
\begin{equation}\label{eq:sqrtm}
    \begin{aligned}
        U = \sqrt{M + W}
    \end{aligned}
    \quad 
    \rightarrow
    \quad 
    \begin{aligned}
        U^TU = M + W,
    \end{aligned}
\end{equation}
for two arbitrary matrices $M$ and $W$. As shown in \todo{cite altro once bib is in}, the
upper-triangular matrix square root of a sum of matrices can be expressed as a function of
each individual matrix square root by using a function $\operatorname{QR_R}$ that takes the
QR decomposition of the input and returns only the upper triangular R. Using this with the
variables introduced in \eqref{eq:sqrtm} results in the following:
\begin{align}
    \sqrt{M + W} &= \operatorname{QR_R}\bigg( \begin{bmatrix} \sqrt{M} \\ \sqrt{W} \end{bmatrix} \bigg)
\end{align}
Using this, the factorization of the $F^TF$ matrix can be updated using the following
recursion:
\begin{align}
    \sqrt{F_{1:i}^TF_{1:i}} = \operatorname{QR_R}\bigg( \begin{bmatrix} \sqrt{F_{1:i-1}^TF_{1:i-1}} \\ F_i \end{bmatrix} \bigg).
\end{align}
The final algorithm for solving a least squares problem in a recursive-batch fashion is
described in algorithm \ref{alg:rlsqr}. This algorithm can be modified to handle L2
regularized least squares problems by simply replacing line 2 of algorithm \ref{alg:rlsqr}
with $U\leftarrow \operatorname{QR_R}(\operatorname{vcat}(F_1,\sqrt{\rho}I))$, where $\rho$
is the regularizer. 
 \begin{algorithm} 
	\begin{algorithmic}[1]
		\caption{Recursive Batch Least Squares with QR}\label{alg:rlsqr}
		\State \textbf{input} $F,\,d$  \Comment{problem data}
		\State $U \leftarrow \operatorname{QR_R}(F_1)$ \Comment{form initial upper-triangular square-root}
		\State $b \leftarrow F_1^Td_1$ \Comment{form initial right hand side vector}
		\For{$i = 2:N$}
% 		\State $U \leftarrow \operatorname{QR_R}([U^T \, A_i^T]^T) $ 
        \State $U \leftarrow \operatorname{QR_R}(\operatorname{vcat}(U,F_i)) $ \Comment{update square-root with new batch}
		\State $b \leftarrow b + F_i^Td_i$ \Comment{update right hand side with new batch}
		\EndFor
		\State \textbf{ouput} \,$x \leftarrow U^{-1}U^{-T}b$ \Comment{forward and backwards substitution to solve for $x$}
	\end{algorithmic}
\end{algorithm}

% \begin{figure}
%   \centering
%   \begin{subfigure}[b]{0.40\textwidth}
%     \includegraphics[width=\textwidth,height=5cm]{cartpole_lqr_stabilization_performance.tikz}
%     \caption{LQR Controllers}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.59\textwidth}
%     \includegraphics[width=\textwidth,height=5cm]{cartpole_mpc_stabilization_performance.tikz}
%     \caption{MPC Controller}
%   \end{subfigure}
%   \label{fig:cartpole_stabilization}
%   \caption{Controller performance on the task of stabilizing the cartpole system about the 
%   upward unstable equilibrium. Performance is plotted as the error }
% \end{figure}
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth, height=4cm]{cartpole_lqr_samples.tikz}
  \caption{Number of training trajectories requires to beat the nominal MPC controller.
    The criteria is the L2 norm of the state from the goal state after 4 seconds.
    The ``Lifted'' MPC controllers compute the MPC solution in the lifted state space 
    (17 states), whereas the ``Projected'' MPC contollers project the Jacobians of the 
    bilinear system back into the original state space.
  }
  \label{fig:cartpole_lqr_samples}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \raggedleft
    \includegraphics[width=\textwidth, height=5cm]{../images/cartpole_mpc_test_error.tikz}
    \caption{MPC tracking error vs training samples for the cartpole. Tracking error is
    defined as the average L2 error over all the test trajectories between the reference and
    simulated trajectories. The proposed jDMD method immediately learns a model good enough
    for control with just a couple hundred dynamics samples (2 swing-up trajectories).}
    \label{fig:cartpole_mpc_test_error}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \raggedright
    \includegraphics[width=\textwidth, height=5cm]{cartpole_mpc_train_time.tikz}
    \caption{Training time for cartpole models as a function of training samples. The 
    training time complexity is approximately linear.}
    \label{fig:cartpole_train_time}
  \end{subfigure}
\end{figure}

\subsubsection{MPC Tracking Performance}

To train the bilinear model for the swing-up task, we generated 50 training swing-up
trajectories using ALTRO, an open-source nonlinear trajectory optimization solver 
\cite{Howell2019,Jackson2021}, on the nominal cartpole model. Each trajectory was 5 seconds
long and sampled at 20 Hz.  A linear MPC controller was then used to track the swing-up
trajectories on the simulated system, resulting in significant tracking error due to the
model mismatch.  After learning both models, a linear MPC policy was used to track the
original swing-up trajectory generated by ALTRO. The MPC policy linearized the dynamics
about the reference trajectory, used a quadratic penalty on deviations from the reference,
and was solved using Riccati recursion for a horizon of 41 time steps (2 seconds). To
successfully stabilize the system at the upward equilibrium, the MPC policy extrapolated the
terminal state and control reference indefinitely. For each of the bilinear models, the
Jacobians were projected back onto space of the original dynamics, resulting in an MPC
policy that was just as efficient to solve online as the nominal model (all controllers ran
at several thousand Hertz without much performance tuning). 

The tracking error, defined as the average L2 norm of the error between the reference 
trajectory and the actual trajectory of the simulated system for 10 test trajectories not 
included in the training data, was recorded after training both the eDMD and jDMD models
with an increasing number of trajectories. The results in Figure
\ref{fig:cartpole_mpc_test_error} clearly show that jDMD produces a high-quality model with
extremely few samples, whereas eDMD---even after many training samples---never achieves the 
same level of performance. The lack of progress with increasing samples is likely a result 
of poor variety in the training data: after enough samples both methods effectively learn 
all the information that can be learned from the distribution from which the training data 
is sampled. This example highlights the value of adding even just a little derivative 
information to a data-driven approach: it dramatically increases sample efficiency while 
also improving the quality of the learned model, especially when that model is used in 
optimization-based controllers such as MPC that rely on derivative information. For 
reference, the training times are shown in Figure \ref{fig:cartpole_train_time}. Note that 
the training algorithms haven't yet been optimized for max performance but reflect a decent 
first implementation.


\subsection{Planar Quadrotor}
As another benchmark example, we use the planar quadrotor model, a simplification of the full quadrotor system with only 3 degrees of freedom. Like the cartpole, we once again specify two separate analytical models used in simulation: the \textit{nominal} model is a simple planar quadrotor system without aerodynamic drag, whereas the "real" \textit{simulated} model includes aerodynamic drag, which can be seen as a sort of viscous damping force. Additionally, we altered the system properties (e.g. mass, rotor arm length, etc.) by ~5\% to incorporate additional mismatch between the nominal and simulated model. See the provided code for the actual values and models used in the experiments.

Analysis of the planar quadrotor system is also split into 2 separate tasks: stabilization
about the hover position with perturbed initial conditions, and a point-to-point translation
where the system must do its best to track an infeasible linear trajectory and stabilize
about hover at the goal position.

\subsubsection{Stabilization}
For stabilizing the system about the upward unstable equilibrium, we used an LQR controller
designed using the nominal planar quadrotor model to collect various trajectories. For learn our bilinear eDMD and jDMD, the following nonlinear mapping was used: $\phi(x) = [\; 1,\; x,\; \sin(x),\; \cos(x),\; 2x^{2}-1\; ] \in \R^{25}$. After learning both models, respective LQR controllers were designed using the bilinear models' ``projected'' Jacobians \eqref{eq:projected_jacobians}. The LQR controllers linearized about hover at the origin ($x = [\;0,\; 0,\; 0,\;, 0\;  0\;  0\;]$) and solved the resulting equality-constrained quadratic program using Riccati recursion. Each controller was tested using 30 different initial
conditions sampled from a uniform distribution of initial conditions centered about the hover position.

To study how robust eDMD and jDMD are to regularization, we trained multiple bilinear eDMD and jDMD models with increasing L2 (Tikhonov) regularization values. LQR controllers were designed for each model and tested with 100 initial conditions sampled from a uniform distribution centered about the hover position. The average L2 error of the state after 5.0 seconds of simulation, which we'll refer to in this section as the stabilization error, was recorded for each model and respective LQR controller. As shown in Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_reg}, the proposed jDMD method remains robust over a range of small regularization values ($<10^{-1}$) when compared to nominal eDMD. This may suggest a need for greater caution when choosing a regularization value when performing eDMD so that stability and controllability is not lost when attempting to reduce overfitting.

As suggested by Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_reg}, eDMD without regularization performs best, but may be prone to overfitting. Therefore, we test eDMD's and jDMD's respective LQR controllers on initial conditions sampled from increasingly larger uniform distributions to analyze the generalizability of the lifted, bilinear models. The LQR controllers are tested on 100 samples generated from the uniform distribution, and the average stabilization error is recorded. As seen in Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_training_window}, jDMD's LQR controller is much more robust than eDMD, even when the initial conditions go beyond the scope of the training data. This suggests that adding Jacobian information not only makes eDMD more sample efficient, but can also decrease overfitting. 

To further study the robustness of eDMD and jDMD, we tasked the respective LQR controllers for eDMD and jDMD to stabilize under increasingly-varying equilibrium positions. 50 equilibrium positions are sampled from a uniform distribution with increasing bounds, which represent the maximum offset from the origin. The samples are used to test the models' LQR controllers before recording the average stabilization error. The results in Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_equilibrium_change} show that jDMD is very robust to the change in equilibrium and is able to successfully stabilize about the equilibrium despite the increasing offset. This is expected because stabilization performance should be invariant with respect to the quadrotor's equilibrium position, and this information is provided by the Jacobians in jDMD. eDMD's increasing inability to stabilize for greater equilibrium offsets is likely a result of eDMD being overfitted to the training trajectories, which all stabilize about the origin. Therefore, this example once again highlights the value of adding Jacobian information: it is not only a data augmentation technique for sample efficiency, but also a regularizer that can decrease overfitting, making the learned models (and subsequent controllers) more robust and generalizable.

\begin{figure}
	\centering
	\includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_reg.tikz}
	\caption{LQR stabilization with equilibrium offset}
	\label{fig:rex_planar_quadrotor_lqr_error_by_reg}
	\caption{ LQR stabilization error over a range of L2 regularization values. jDMD is much more robust and consistent over small regularization values.}
\end{figure}

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_equilibrium_change.tikz}
		\caption{LQR stabilization error over increasing equilibrium offset}
		\label{fig:rex_planar_quadrotor_lqr_error_by_equilibrium_change}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_training_window.tikz}
		\caption{LQR stabilization error over varying range of distribution for sampling initial conditions. A training range fraction greater than $1$ indicates the distribution range is beyond that used to generate the training trajectories}
		\label{fig:rex_planar_quadrotor_lqr_error_by_training_window}
	\end{subfigure}
	\caption{ LQR controller robustness to varying initial conditions and equilibrium offset. jDMD is much more robust overall with an ability to stabilize outside the scope of training data }
\end{figure}

\subsubsection{MPC Tracking Performance}

To train the bilinear model for the MPC tracking task, we generated 50 infeasible point-point
linear trajectories that all ended at the origin with the planar quadrotor in hover. The starting
initial condition of the system was sampled from a uniform distribution about the origin.
A linear MPC controller was then used to track the infeasible linear trajectories on the
simulated system, resulting in significant tracking error due to the model mismatch. 
After learning both models, a linear MPC policy (identical to the cartpole example) was
used to track the original linear trajectory as best as possible.

The tracking error we recorded for 35 test trajectories after training both the eDMD 
and jDMD models with an increasing number of trajectories. Like the cartpole example,
results in Figure \ref{fig:rex_planar_quadrotor_mpc_test_error} clearly show that jDMD
produces a high-quality model for MPC trackign with extremely few samples, whereas eDMD
---even after many training samples---is never able to develop a model with consistent
and similar performance. The lack of progress with increasing samples is likely a result
of poor variety in the training data: the jDMD model is not receiving any new information
from additional training trajectories, and eDMD is not able to capture enough data that
can capture enough behaviors that lie in the testing space. This is once again highlighted
in the results of Figure \ref{fig:rex_planar_quadrotor_mpc_error_by_training_window}, which
show that eDMD is only able to successfully track trajectories before falling apart when the
range of test trajectories exceeds that of the training data. Meanwhile, jDMD is able to consistently
track the trajectory to the goal state, despite having greater tracking error in a certain range
of trajectories---this further hints at eDMD overfitting on the training data.

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\raggedleft
		\includegraphics[width=\textwidth, height=5cm]{rex_planar_quadrotor_mpc_test_error.tikz}
		\caption{MPC tracking error vs training samples for the planar quadrotor. The proposed jDMD method again learns a model good enough with ~1000 samples while eDMD is never able to converge to a good-enough model.}
		\label{fig:rex_planar_quadrotor_mpc_test_error}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\raggedright
		\includegraphics[width=\textwidth, height=5cm]{rex_planar_quadrotor_mpc_error_by_training_window.tikz}
		\caption{MPC tracking error of quadrotor over varying range of distribution for sampling initial conditions. Drastically increasing error for eDMD suggests greater overfitting.}
		\label{fig:rex_planar_quadrotor_mpc_error_by_training_window}
	\end{subfigure}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Limitations 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations} \label{sec:limitations}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion} \label{sec:conclusion}

\bibliography{references.bib}

\end{document}