\documentclass{article}
% \usepackage{corl_2022} % Use this for the initial submission.
\usepackage[final]{corl_2022} % Uncomment for the camera-ready ``final'' version.
\usepackage{times} % assumes new font selection scheme installed
\usepackage{brian}
\graphicspath{{../images/}}
\usepackage[export]{adjustbox}
% \usepackage{biblatex}

% Bibliography
% \usepackage[style=ieee,doi=false,isbn=false,url=false,eprint=false]{biblatex}
% \AtEveryBibitem{\clearlist{language}}  % remove language field from bib entries
% \addbibresource{references.bib}
% \renewcommand*{\bibfont}{\small}


\title{Extended Dynamic Mode Decomposition with Jacobian Residual Penalization
for Learning Bilinear, Control-affine Koopman Models}

\author{
  Brian E. Jackson \\
  Robotics Institute \\
  Carnegie Mellon University\\
  \texttt{brianjackson@cmu.edu} \\
  \and
  Jeong Hun Lee \\
  Robotics Institute\\
  Carnegie Mellon University\\
  \texttt{jeonghunlee@cmu.edu} \\
  \and
  Kevin Tracy \\
  Robotics Institute\\
  Carnegie Mellon University\\
  \texttt{ktracy@cmu.edu} \\
  \and
  Zachary Manchester \\
  Robotics Institute\\
  Carnegie Mellon University\\
  \texttt{zacm@cmu.edu} \\
}

\begin{document}
\maketitle

\todo{Add abstract}

\section{Introduction}

    Controlling complex, underactuated, and highly nonlinear autonomous systems remains an
    active area of research in robotics, despite decades of previous work exploring
    effective algorithms and the development of substantial theoretical analysis. Classical
    approaches typically rely on local linear approximations of the nonlinear system, which
    are then used in any of a multitude of linear control techniques, such as PID, pole
    placement, Bode analysis, H-infinity, LQR, or linear MPC.  These approaches only work
    well if the states of the system always remain close to the linearization point or
    reference trajectory. The region for which these linearizations remain valid can be
    extremely small for highly nonlinear systems.  Alternatively, model-based methods for
    nonlinear control based on optimal control have shown great success, as long as the
    model is well known and an accurate estimate of the global state can be provided. These
    model-based techniques leverage centuries of insight into dynamical systems and have
    demonstrated incredible performance on extremely complicated autonomous systems 
    \cite{farshidian_efficient_2017,Kuindersma2014,Bjelonic2021,Subosits2019} .  On the
    other hand, data-driven techniques such as reinforcement learning have received
    tremendous attention over the last decade and have begun to demonstrate impressive
    performance and robustness for complicated robotic systems in unstructured environments
    \cite{Karnchanachari2020,Hoeller2020,Li2021}. While these approaches are attractive
    since they require little to no previous knowledge about the system, they often require
    large amounts of data and fail to generalize outside of the domain or task on which they
    were ``trained.''
    
    In this work we propose a novel method that combines the benefits of model-based and
    data-driven methods, based on recent work  applying Koopman Operator Theory to
    controlled dynamical systems 
    \cite{Meduri2022,Bruder2021,Korda2018,Folkestad2020,Suh2020}.
    % \cite{Meduri2022,Bruder2021, Korda2021}
    % \cite{Meduri2022, Folkestad2021, Bruder2021, Korda2018, Folkestad2020a, Folkestad2020b}.  
    By leveraging data collected from an
    unknown dynamical system along with derivative information from an approximate
    analytical model, we can efficiently learn a bilinear representation of the system
    dynamics that performs well when used in traditional model-based control techniques such
    as linear MPC. The result is nonlinear dynamical system that is expressive enough to
    capture the full nonlinear dynamics across a broad range of the state space, but has a
    well-defined structure that is very amenable to specialized optimization-based
    controllers. By leveraging information from an analytical model, we can dramatically
    reduce the number of samples required to learn a good approximation of the true
    nonlinear dynamics. 
    
    \todo{bullet out contributions.}
    
    \todo{add summary of paper layout.}

\section{Background and Related Work} \label{sec:Preliminaries/Background}

  \subsection{The Koopman Operator}
  The theoretical underpinnings of the Koopman operator and its application to dynamical
  systems has been extensively studied, especially within the last decade 
  \cite{Fasel2021,Proctor2018,Bruder2021,Williams2015}. Rather than describe the theory in
  detail, we highlight the key concepts employed by the current work, and defer the
  motivated reader to the existing literature on Koopman theory.

  We start by assuming we have some discrete approximation a of controlled nonlinear,
  time-dynamical system whose underlying continuous dynamics are Lipschitz continuous:
  \begin{equation} \label{eq:discrete_dynamics} x_{k+1} = f(x_k, u_k) \end{equation} where
  $x \in \mathcal{X} \subseteq \R^{N_x}$ is the state vector and $u_k \in \R^{N_u}$ is the
  control vector.  This discrete approximation can be obtained for any continuous dynamical
  system in many ways, including implicit and explicit Runge-Kutta methods, or by solving
  the Discrete Euler-Lagrange equations \cite{Brudigam2021a,Brudigam2021,Howell2022}.

  The key idea behind the Koopman operator is that the nonlinear finite-dimensional discrete
  dynamics \eqref{eq:discrete_dynamics} can be represented by an infinite-dimensional
  \textit{bilinear} system:
  \begin{equation} \label{eq:bilinear_dynamics}
      y_{k+1} = A y_k + B u_k + \sum_{i=1}^m u_{k,i} C_i y_k = g(y,u)
  \end{equation}
  where $y = \phi(x)$ is a mapping from the finite-dimensional state space $\mathcal{X}$ to
  the (possibly) infinite-dimensional Hilbert space of \textit{observables} $y \in
  \mathcal{Y}$. We also assume the inverse map is approximately linear: $x = G y$. In
  practice, we approximate \eqref{eq:discrete_dynamics} by choosing $\phi$ to be some
  arbitrary finite set of nonlinear functions of the state variables, which in general
  include the states themselves such that the linear mapping $G \in \R^{N_x \times N_y}$ is
  exact.  Intuitively, $\phi$ ``lifts'' our states into a higher dimensional space where the
  dynamics are approximately (bi)linear, effectively trading dimensionality for
  (bi)linearity. This idea should be both unsurprising and familiar to most roboticsts,
  since similar techniques have already been employed in other forms, such as
  maximal-coordinate representations of rigid body dynamics
  \cite{baraff_linear-time_1996-1,Brudigam2021a,Howell2022}, the
  ``kernel trick'' for state-vector machines \cite{Hofmann2006}, or the observation that
  solving large, sparse nonlinear optimization problems is often more effective than solving
  small, tightly-coupled dense problems \todo{add citations from the trajectory optimization
  literature}.

  The lifted bilinear system \eqref{eq:bilinear_dynamics} can be easily learned from samples
  of the system dynamics $(x_i^+,x_i,u_i)$ (where $x^+$ is the state at the next time step)
  using extended Dynamic Mode Decomposition (eDMD), which is just the application of linear
  least squares (LLS) to the lifted states.  Details of this method will be covered later
  when we introduce our adaptation of eDMD and present an effective numerical technique for
  solving the resulting LLS problems.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{EDMD with Jacobian Residual-Penalization} \label{sec:methodology}
  Existing Koopman-based approaches to learning dynamical systems only rely on samples of
  the unknown dynamics. Here we present a novel method for incorporating prior knowledge
  about the dynamics by adding derivative information of an approximate model into the data
  set to be learned via eDMD.

  Given $P$ samples of the dynamics $(x_i^+, x_i, u_i)$, and an approximate discrete
  dynamics model 
  \begin{equation}
      x^+ = \tilde{f}(x,u)
  \end{equation}
  we can evaluate the Jacobians of our approximate model $\hat{f}$ at each of the sample
  points: $\tilde{A}_i = \pdv{\tilde{f}}{x}, \tilde{B}_i = \pdv{\tilde{f}}{x}$. After
  choosing nonlinear mapping $\phi : \R^{N_x} \mapsto \R^{N_y}$ We then want to find a
  bilinear dynamics model \eqref{eq:bilinear_dynamics} that matches the Jacobians of our
  approximate model, while also matching our dynamics samples. If we define $\hat{A}_j \in
  \R^{N_x \times N_x}$ and $\hat{B}_j \in \R^{N_x \times N_u}$ to be the Jacobians of our
  bilinear dynamics model, projected back into the original state space, our objective is to
  find the matrices $A \in \R^{N_y \times N_y},B \in \R^{N_y \times N_u}$, and $C_{1:m} \in
  \R^{N_u} \times \R^{N_y \times N_y}$ that minimize the following objective:

  \begin{equation} \label{eq:lls_objective}
      (1- \alpha) \sum_{j=1}^P \norm{\hat{y}_j - y_j^+}_2^2 + 
          \alpha  \sum_{j=1}^P \norm{\hat{A}_j - \tilde{A}_j}_2^2 + 
                               \norm{\hat{B}_j - \tilde{B}_j}_2^2 
  \end{equation}
  where $\hat{y}_j^+ = g\left(\phi(x_j), u_j\right)$ is the output of our bilinear  dynamics
  model, and $y_j^+ = \phi(y_j^+)$ is the actual lifted state (i.e. observables) at the next
  time step.

  While not immediately apparent, we can minimize \eqref{eq:lls_objective} using linear
  least-squares, using techniques similar to those used previously in the literature
  \cite{Folkestad2021}.

  To start, we combine all the data we're trying to learn in a single matrix:
  \begin{equation}
      E = \begin{bmatrix} A & B & C_1 & \dots & C_m \end{bmatrix} \in \R^{N_y \times N_z} \\
  \end{equation}
  where $N_z = N_y + N_u + N_y \cdot N_u$.  We now rewrite the terms in
  \eqref{eq:lls_objective} in terms of $E$. By defining the vector 
  \begin{equation}
      z = \begin{bmatrix} y^T & u^T & u_1 y^T & \dots & u_m y^T \end{bmatrix} \in \R^{N_z} 
  \end{equation}
  we can write down 
  the output of our bilinear dynamics \eqref{eq:bilinear_dynamics} as 
  \begin{equation} \label{eq:bilinear_dynamics_z}
      \hat{y}^+ = E z.
  \end{equation}
  The projected Jacobians of our bilinear model, $\hat{A}$ and $\hat{B}$, are simply the Jacobians 
  of the bilinear dynamics in terms of the original state. We obtain these dynamics by ``lifting`` 
  the state via $\phi$ and then projecting back onto the original states using $G$:
  \begin{equation} \label{eq:projected_dynamics}
      x^+ = G \left( A \phi(x) + B u + \sum_{i=1}^m u_i C_i \phi(x) \right)  = \hat{f}(x,u) 
  \end{equation}
  Differentiating these dynamics give us our projected Jacobians:
  \begin{subequations} \label{eq:projected_jacobians}
  \begin{align}
      \hat{A}_j &= G \pdv{\hat{f}}{x}\left(x_j,u_j\right) 
                = G \left(A + \sum_{i=1}^m u_{j,i} C_i \right) \Phi(x_j)
              %   = G A_j^x \phi(x_j)
                = G E \bar{A}(x_j,u_j) = G E \bar{A}_j \\
      \hat{B}_i &= G \pdv{\hat{f}}{u}\left(x_j,u_j\right) 
                = G \Big(B + \begin{bmatrix} C_1 x_j & \dots & C_m x_j \end{bmatrix} \Big)
              %   = G B_j^u
                = G E \bar{B}(x_j,u_j) = G E \bar{B}_j
  \end{align}
  \end{subequations}
  where $\Phi(x) = \pdv*{\phi}{x}$ is the Jacobian of the nonlinear map $\phi$, and
  \begin{equation}
      \bar{A}(x,u) =  \begin{bmatrix} 
          I \\ 0 \\ u_1 I \\ u_2 I \\ \vdots \\ u_m I 
      \end{bmatrix} \in \R^{N_z \times N_x}, \quad
      \bar{B}(x,u) = \begin{bmatrix} 
          0 \\ 
          I \\ 
          [x \; 0 \; ... \; 0] \\
          [0 \; x \; ... \; 0] \\
          \vdots \\
          [0 \; 0 \; ... \; x] \\
      \end{bmatrix} \in \R^{N_z \times N_u}.
  \end{equation}
  Substituting \eqref{eq:bilinear_dynamics_z} and \eqref{eq:projected_jacobians} into
  \eqref{eq:lls_objective}, we can rewrite our LLS problem as:
  \begin{align}
      \underset{E}{\text{minimize}} \;\; 
          \sum_{j=0}^P
          (1-\alpha) \norm{E z_j - y_j^+}_2^2 + 
            \alpha  \norm{G E \bar{A}_j - \tilde{A}_j}_2^2 + 
            \alpha  \norm{G E \bar{B}_j - \tilde{B}_j}_2^2 
  \end{align}
  which is equivalent to
  \begin{align} \label{opt:lls_matrix}
      \underset{E}{\text{minimize}} \;\; 
          (1-\alpha) \norm{E \mathbf{Z_{1:P}} - \mathbf{Y^+_{1:P}} }_2^2 + 
            \alpha  \norm{G E \mathbf{\bar{A}_{1:P}} - \mathbf{\tilde{A}_{1:P}}}_2^2 + 
            \alpha  \norm{G E \mathbf{\bar{B}_{1:P}} - \mathbf{\tilde{B}_{1:P}}}_2^2
  \end{align}
  where $\mathbf{Z_{1:P}} \in \R^{N_z \times P} = [z_1 \; z_2 \; ... \; z_P]$ horizontally
  concatenates all of the samples (equivalent definition for 
  $\mathbf{Y^+_{1:P}} \in \R^{N_y \times P}$, 
  $\mathbf{\bar{A}_{1:P}} \in \R^{N_z \times N_x \cdot P}$, 
  $\mathbf{\tilde{A}_{1:P}} \in \R^{N_z \times N_x \cdot P}$,
  $\mathbf{\bar{B}_{1:P}} \in \R^{N_z \times N_u \cdot P}$, and 
  $\mathbf{\tilde{B}_{1:P}} \in \R^{N_z \times N_u \cdot P}$ ).

  We can rewrite \eqref{opt:lls_matrix} in standard form using the ``vec trick''
  \begin{equation} \label{eq:vectrick}
      \text{vec}(A X B) = (B^T \otimes A) \text{vec}(X)
  \end{equation}
  where $\text{vec}(A)$ stacks the columns of $A$ into a single vector.

  Setting $E$ in \eqref{opt:lls_matrix} equal to $X$ in \eqref{eq:vectrick}, we get
  \begin{align} \label{opt:lls_matrix}
      \underset{E}{\text{minimize}} \;\;  
      \norm{
          \begin{bmatrix}
              (1-\alpha)\cdot(\mathbf{Z_{1:P}})^T \otimes I_{N_y} \\
              \alpha\cdot(\mathbf{\bar{A}_{1:P}})^T \otimes G \\
              \alpha\cdot(\mathbf{\bar{G}_{1:P}})^T \otimes G \\
          \end{bmatrix}
          \text{vec}(E)
          +
          \begin{bmatrix}
              (1-\alpha)\cdot\text{vec}(\mathbf{Y^+_{1:P}}) \\
              \alpha\cdot\text{vec}(\mathbf{\tilde{A}_{1:P}}) \\
              \alpha\cdot\text{vec}(\mathbf{\tilde{G}_{1:P}})
          \end{bmatrix}
      }_2^2
  \end{align}
  such that the matrix of cofficients has $(N_y + N_x^2 + N_x \cdot N_u) \cdot P$ rows and 
  $N_y \cdot N_z$ columns.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results} \label{sec:results}
All continuous dynamics were discretized with an explicit fourth-order Runge Kutta 
integrator.

\subsection{Training}

All models were trained by simulating the ``real'' system with an arbitrary controller to 
collect data in the region of the state space relevant to the task. A set of fixed-length 
trajectories were collected, each at a sample rate of 20 Hz. The bilinear eDMD model was
trained using the same approach in \cite{Folkestad2021}. For the proposed jDMD method, the
Jacobians of the nominal model were calculated at each of the sample points and the bilinear
model was learned using the approach outlined in Section \ref{sec:methodology}.

\subsection{Cartpole}
As a simple benchmark example, we use the canonical cartpole system. In lieu of an actual 
hardware experiment (left for future work), we specify two separate analytical models used 
in simulation: the \textit{nominal} model is a simple cartpole system without friction or 
damping, whereas the \textit{simulated} model, used exclusively for simulating the system
in place of a real hardware platform, includes viscous damping on both degrees of freedom,
a tanh Coloumb friction model for the cart, and a deadband on the control signal. 
Additionally, we altered the mass of the cart and pole from the nominal model by 20\% and 
25\%, respectively. See the provided code for the actual values and models used in the 
experiments.

We split the analysis of this system into two separate tasks: stabilization about the upward
unstable equilibrium from perturbed initial condtions, and the swing-up task where the 
system must successfully stabilize after starting from the downward equilibrium.

\subsubsection{Stabilization}

For stabilizing the system about the upward unstable equilibrium we used an LQR controller
designed using to nominal model to collect trajectories on the simulated system.  To learn
the bilinear models, we used the following nonlinear mapping: $\phi(x) = [\; 1,\; x,\;
\sin(x),\; \cos(x),\; \sin(2x)\; ] \in \R^{17}$.  After learning both models, an MPC
controller was designed using the nominal, eDMD, and jDMD models. For the learned bilinear
models, MPC controllers were designed using the ``lifted'' Jacobians of the bilinear 
dynamics and the ``projected'' Jacobians \eqref{eq:projected_jacobians}. The MPC controllers
linearized the dynamics about the equilibrium of $x = [\;0,\; \pi,\; 0,\;, 0\;]$ and solved
the resulting equality-constrained quadratic program using Riccati recursion and a horizon
of 41 time steps (2 seconds). 

To analyze sample efficiency of the algorithms, we trained the bilinear models with an 
increasing number of samples.  Each controller was tested using 100 different initial
conditions sampled from a uniform distribution of initial conditions centered about the 
upward equilibirum. The average L2 error of the state after 4 seconds and the upward 
equilibrium was recorded for each controller. The minimum number of training trajectories to
get performance better than the nominal MPC controller is reported in Figure 
\ref{fig:cartpole_lqr_samples}. As shown, the proposed approach is more sample efficient 
even for this relatively simple task than traditional eDMD. It also shows the benefit of 
applying the control in the original state space by projecting the linearization back to the
original states. To our knowledge, no previous works on applying DMD or Koopman operator 
theory to controlled systems have used this technique of projecting back into the original 
state space, although the benefits are immediately apparent: with just a few training 
trajectories (3 for jDMD and 7 for eDMD) we can learn a model that improves upon our nominal
LQR controller policy.

% \begin{figure}
%   \centering
%   \begin{subfigure}[b]{0.40\textwidth}
%     \includegraphics[width=\textwidth,height=5cm]{cartpole_lqr_stabilization_performance.tikz}
%     \caption{LQR Controllers}
%   \end{subfigure}
%   \begin{subfigure}[b]{0.59\textwidth}
%     \includegraphics[width=\textwidth,height=5cm]{cartpole_mpc_stabilization_performance.tikz}
%     \caption{MPC Controller}
%   \end{subfigure}
%   \label{fig:cartpole_stabilization}
%   \caption{Controller performance on the task of stabilizing the cartpole system about the 
%   upward unstable equilibrium. Performance is plotted as the error }
% \end{figure}
\begin{figure}
  \centering
  \includegraphics[width=0.7\textwidth, height=4cm]{cartpole_lqr_samples.tikz}
  \caption{Number of training trajectories requires to beat the nominal MPC controller.
    The criteria is the L2 norm of the state from the goal state after 4 seconds.
    The ``Lifted'' MPC controllers compute the MPC solution in the lifted state space 
    (17 states), whereas the ``Projected'' MPC contollers project the Jacobians of the 
    bilinear system back into the original state space.
  }
  \label{fig:cartpole_lqr_samples}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \raggedleft
    \includegraphics[width=\textwidth, height=5cm]{../images/cartpole_mpc_test_error.tikz}
    \caption{MPC tracking error vs training samples for the cartpole. Tracking error is
    defined as the average L2 error over all the test trajectories between the reference and
    simulated trajectories. The proposed jDMD method immediately learns a model good enough
    for control with just a couple hundred dynamics samples (2 swing-up trajectories).}
    \label{fig:cartpole_mpc_test_error}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \raggedright
    \includegraphics[width=\textwidth, height=5cm]{cartpole_mpc_train_time.tikz}
    \caption{Training time for cartpole models as a function of training samples. The 
    training time complexity is approximately linear.}
    \label{fig:cartpole_train_time}
  \end{subfigure}
\end{figure}

\subsubsection{MPC Tracking Performance}

To train the bilinear model for the swing-up task, we generated 50 training swing-up
trajectories using ALTRO, an open-source nonlinear trajectory optimization solver 
\cite{Howell2019,Jackson2021}, on the nominal cartpole model. Each trajectory was 5 seconds
long and sampled at 20 Hz.  A linear MPC controller was then used to track the swing-up
trajectories on the simulated system, resulting in significant tracking error due to the
model mismatch.  After learning both models, a linear MPC policy was used to track the
original swing-up trajectory generated by ALTRO. The MPC policy linearized the dynamics
about the reference trajectory, used a quadratic penalty on deviations from the reference,
and was solved using Riccati recursion for a horizon of 41 time steps (2 seconds). To
successfully stabilize the system at the upward equilibrium, the MPC policy extrapolated the
terminal state and control reference indefinitely. For each of the bilinear models, the
Jacobians were projected back onto space of the original dynamics, resulting in an MPC
policy that was just as efficient to solve online as the nominal model (all controllers ran
at several thousand Hertz without much performance tuning). 

The tracking error, defined as the average L2 norm of the error between the reference 
trajectory and the actual trajectory of the simulated system for 10 test trajectories not 
included in the training data, was recorded after training both the eDMD and jDMD models
with an increasing number of trajectories. The results in Figure
\ref{fig:cartpole_mpc_test_error} clearly show that jDMD produces a high-quality model with
extremely few samples, whereas eDMD---even after many training samples---never achieves the 
same level of performance. The lack of progress with increasing samples is likely a result 
of poor variety in the training data: after enough samples both methods effectively learn 
all the information that can be learned from the distribution from which the training data 
is sampled. This example highlights the value of adding even just a little derivative 
information to a data-driven approach: it dramatically increases sample efficiency while 
also improving the quality of the learned model, especially when that model is used in 
optimization-based controllers such as MPC that rely on derivative information. For 
reference, the training times are shown in Figure \ref{fig:cartpole_train_time}. Note that 
the training algorithms haven't yet been optimized for max performance but reflect a decent 
first implementation.

\subsection{Planar Quadrotor}
As another benchmark example, we use the planar quadrotor model, a simplification of the full quadrotor system with only 3 degrees of freedom. Like the cartpole study, we once again specify two separate analytical models used in simulation: the \textit{nominal} model is a simple planar quadrotor system without aerodynamic drag, whereas the \textit{simulated} model, used exclusively for simulating the system
in place of a real hardware platform, includes aerodynamic drag, which acts as a viscous damping force. 
Additionally, we altered the system properties (e.g. mass, rotor arm length, etc.) by ~5\% to incorporate additional mismatch between the nominal and simulated model. See the provided code for the actual values and models used in the experiments.

\subsubsection{Stabilization}

For stabilizing the system about the upward unstable equilibrium 50 initial conditions were
collected using an LQR controller designed using the nominal model. The a basis function of 
$\phi(x) = [\; 1,\; x,\; \sin(x),\; \cos(x),\; \sin(2x)\; ] \in \R^{17}$ was used. After
learning both models, LQR and MPC controllers were designed using the nominal, eDMD, and
jDMD model. For the learned bilinear models, a controller was designed using both the  
Jacobians of the bilinear dynamics and the projected Jacobians \eqref{eq:projected_jacobians}. The MPC controllers linearized the dynamics about the equilibrium and solved the resulting equality-constrained quadratic program using Riccati recursion and a horizon of 41 time steps (2 seconds). Each controller was tested using 100 different initial conditions. The distance of the state after 4 seconds for the LQR and MPC controllers is shown in Figure

\begin{figure}
	\centering
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_equilibrium_change.tikz}
		\caption{LQR stabilization with equilibrium offset}
	\end{subfigure}
	\begin{subfigure}[t]{0.45\textwidth}
		\includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_training_window.tikz}
		\caption{LQR stabilization with varying "window" of initial condition sampling}
	\end{subfigure}
	\label{fig:cartpole_stabilization}
	\caption{ LQR controller robustness to varying initial conditions and equilibrium offset }
\end{figure}

\begin{figure}
	\centering
	\includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_reg.tikz}
	\caption{LQR stabilization with equilibrium offset}
	\label{fig:cartpole_stabilization}
	\caption{ Robustness of jDMD and eDMD to regularization value. jDMD much more robust }
\end{figure}

\begin{figure}[t]
	\centering
	\begin{subfigure}[t]{0.48\textwidth}
		\raggedleft
		\includegraphics[width=\textwidth, height=5cm]{rex_planar_quadrotor_mpc_test_error.tikz}
		\caption{MPC tracking error vs training samples for the cartpole. Tracking error is
			defined as the average L2 error over all the test trajectories between the reference and
			simulated trajectories. The proposed jDMD method immediately learns a model good enough
			for control with just a couple hundred dynamics samples (2 swing-up trajectories).}
		\label{fig:cartpole_mpc_test_error}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.48\textwidth}
		\raggedright
		\includegraphics[width=\textwidth, height=5cm]{rex_planar_quadrotor_mpc_error_by_training_window.tikz}
		\caption{MPC tracking error of quadrotor for various ranges of which to sample initial conditions. Percentage is determined by the limits of the range of training data, where 100\% is the limit of the training data's range}
		\label{fig:cartpole_train_time}
	\end{subfigure}
\end{figure}

\subsubsection{MPC Tracking Performance}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Limitations 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliography{references.bib}

\end{document}