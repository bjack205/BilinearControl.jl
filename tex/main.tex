\documentclass{article}
\usepackage{corl_2022} % Use this for the initial submission.
% \usepackage[final]{corl_2022} % Uncomment for the camera-ready ``final'' version.
\usepackage{times} % assumes new font selection scheme installed
\usepackage{brian}
\graphicspath{{../images/}}
\usepackage[export]{adjustbox}
\usepackage{wrapfig}
\usepackage{xcolor}
% \usepackage{biblatex}

% Bibliography
% \usepackage[style=ieee,doi=false,isbn=false,url=false,eprint=false]{biblatex}
% \AtEveryBibitem{\clearlist{language}}  % remove language field from bib entries
% \addbibresource{references.bib}
% \renewcommand*{\bibfont}{\small}


\title{
  % JDMD: Leveraging Analytic Jacobians to Quickly Learn Effective MPC Controllers
  % JDMD: A Sample-Efficient Method for Learning How To Control Nonlinear Systems
  % JDMD: Boosting Sample Efficiency in Bilinear Koopman Models through Derivative Information
  % JDMD: Quickly Boost MPC Performance via Koopman Operators and Approximate Derivatives
  % JDMD: Adapting Adapting Extended Dynamic Mode Decomposition to Leverage What You Know 
  Data-Efficient Model Learning for Control with Jacobian-Regularized Dynamic Mode Decomposition}

\author{
Brian E. Jackson \\
Robotics Institute \\
Carnegie Mellon University \\
\texttt{brianjackson@cmu.edu} \\
\and
Jeong Hun Lee \\
Robotics Institute \\
Carnegie Mellon University \\
\texttt{jeonghunlee@cmu.edu} \\
\and
Kevin Tracy \\
Robotics Institute \\
Carnegie Mellon University \\
\texttt{ktracy@cmu.edu} \\
\and
Zachary Manchester \\
Robotics Institute \\
Carnegie Mellon University \\
\texttt{zacm@cmu.edu} \\
}

\begin{document}
\maketitle

\begin{abstract}
  % Combining the benefits of data-driven and model-based control techniques for nonlinear
  % dynamics systems has many potential advantages. Compared to purely data-driven approaches,
  % this could include benefits such as increasing sampling efficiency, reducing training
  % time, or increasing generalization. For purely model-based approaches such as
  % model-predictive control, this could in- crease robustness to model uncertainty or improve
  % overall performance by adapt- ing the controller based on observations from the real
  % system. 
  We present a novel algorithm for learning Koopman models of controlled nonlinear dynamical
  systems from data based on Dynamic-Mode Decomposition (DMD).  Our approach,
  Jacobian-Regularized DMD (JDMD), offers dramatically improved sample efficiency over
  existing DMD-based algorithms by leveraging Jacobian information from an approximate prior
  model of the system.  We demonstrate JDMD's ability to quickly learn bilinear Koopman
  dynamics representations across several realistic examples in simulation, including a
  quadrotor and a perching fixed-wing aircraft.  In all cases, we show that the models
  learned by JDMD provide superior tracking and generalization performance in a
  model-predictive control framework when compared to both the approximate prior models used
  in training and models learned by standard extended DMD.
\end{abstract}

\section{Introduction}

In recent years, both model-based optimal-control \cite{Farshidian2017,Kuindersma2014,Bjelonic2021,Subosits2019} and data-driven reinforcement-learning methods \cite{Karnchanachari2020,Hoeller2020,Li2021} have demonstrated impressive successes on complex, nonlinear robotic systems. However, both of approaches suffer from inherent drawbacks: Data-driven methods often require extremely large amounts of data and fail to generalize outside of the domain or task on which they were trained. On the other hand, model-based methods require an accurate model of the system to achieve good performance. In many cases, high-fidelity models can be too difficult to construct from first principles or too computationally expensive to be of practical use. However, low-order approximate models that can be evaluated cheaply at the expense of controller performance are often available. With this in mind, we seek a middle ground between model-based and data-driven approaches in this work.

We propose a method for learning bilinear Koopman models \todo{cite stuff} of nonlinear
dynamical systems for use in model-predictive control that leverages information from an
approximate prior dynamics model of the system in the training process.  Our new algorithm
builds on extended Dynamic Mode Decomposition (EDMD), which learns Koopman models from
trajectory data \cite{Meduri2022,Bruder2021,Korda2018,Folkestad2020,Suh2020}, by adding a
derivative regularization term based on derivatives computed from a prior model.  We show
that this new algorithm, Jacobian-regularized Dynamic Mode Decomposition (JDMD), can learn
models with dramatically fewer samples than EDMD, even when the prior model differs
significantly from the true dynamics of the system.  We also demonstrate the effectiveness
of these learned models in a model-predictive control (MPC) framework.
%on these systems when the learned bilinear system is linearized and projected back into the
%original state space.
The result is a fast, robust, and sample-efficient pipeline for quickly training a model
that can outperform previous Koopman-based MPC approaches as well as purely model-based
controllers that do not leverage data collected from the actual system.
%To learn these bilinear representations, we also propose a numerical technique that allows
%for large systems to be trained while limiting the peak memory required to solve the
%least-squares problem.

Our work is most closely related to the recent work of Folkestad et. al.
\cite{Folkestad2020,Folkestad2021,Folkestad2021a}, which learn bilinear models and apply
nonlinear model-predictive control directly on the learned bilinear dynamics. Other recent
works have combined linear Koopman models with model-predictive control \cite{Korda2018} and
Lyapunov control techniques with bilinear Koopman \cite{Narasingam2022}. Our contributions
are:

\begin{itemize}
  \item A novel extension to extended dynamic mode decomposition, called JDMD, that
  incorporates gradient information from an approximate analytic model
  
  \item A recursive, batch QR algorithm for solving the least-squares problems that arise 
  when learning bilinear dynamical systems using DMD-based algorithms, including JDMD and EDMD
  
  \item A simple linear MPC control technique for learned bilinear control systems that is
  computationally efficient and, when combined with JDMD, requires very little training data
  to achieve good performance
\end{itemize}

The remainder of the paper is organized as follows: In Section
\ref{sec:Preliminaries/Background} we provide some background on the application of Koopman
operator theory to controlled dynamical systems and review some related works.  Section
\ref{sec:jdmd} then describes the proposed JDMD algorithm.  In Section \ref{sec:rls} we
outline a memory-efficient technique for solving the large, sparse linear least-squares
problems that arise when applying JDMD and other DMD-based algorithms.  Next, in Section
\ref{sec:projected_mpc}, we propose an efficient model-predictive control technique that
utilizes the learned bilinear models produced by JDMD.  Section \ref{sec:results} then
provides simulation results and analysis of the proposed algorithm applied to control tasks
on a cartpole, a quadrotor, and a small foam airplane, all subject to significant model
mismatch.  In Section \ref{sec:limitations} we discuss the limitations of our approach,
followed by some concluding remarks in Section \ref{sec:conclusion}.

\section{Background and Related Work} \label{sec:Preliminaries/Background}

\subsection{Koopman Operator Theory}

The theoretical underpinnings of the Koopman operator and its application to dynamical
systems has been extensively studied \cite{Fasel2021,Proctor2018,Bruder2021,Williams2015}.
Rather than describe the theory in detail, we highlight the key concepts employed by the
current work and refer the reader to the existing literature on Koopman theory for further
details.

We start by assuming a controlled, nonlinear, discrete-time dynamical system,
\begin{equation} \label{eq:discrete_dynamics} 
  x^+ = f(x, u), 
\end{equation} 
where $x \in \mathcal{X} \subseteq \R^{N_x}$ is the state vector, $u_k \in \R^{N_u}$ is the
control vector, and $x^+$ is the state at the next time step.  The key idea behind the
Koopman operator is that the nonlinear finite-dimensional dynamics
\eqref{eq:discrete_dynamics} can be represented \emph{exactly} by an infinite-dimensional
bilinear system of the form,
\begin{equation} \label{eq:bilinear_dynamics}
  y^+ = A y + B u + \sum_{i=1}^m u_i C_i y = g(y,u) ,
\end{equation}
where $y = \phi(x)$ is a nonlinear mapping from the finite-dimensional state space
$\mathcal{X}$ to the infinite-dimensional Hilbert space of \textit{observables}
$\mathcal{Y}$.  In practice, we approximate \eqref{eq:bilinear_dynamics} by restricting
$\mathcal{Y}$ to be a finite-dimensional vector space, in which case $\phi$ becomes a
finite-dimensional nonlinear function of the state variables that must be chosen by the
user.


Intuitively, $\phi$ ``lifts'' our state $x$ into a higher dimensional space $\mathcal{Y}$
where the dynamics are approximately (bi)linear, effectively trading dimensionality for
(bi)linearity. Similarly, we can perform the inverse operation by projecting a lifted state
$y$ back into the original state space $\mathcal{X}$. In this work, we will assume that
$\phi$ is constructed in such a way that this inverse mapping is linear:
\begin{equation}
	x = G y
\end{equation}

%This idea should be both unsurprising and familiar to most roboticists, since similar techniques have already been employed in other forms, such as maximal-coordinate representations of rigid body dynamics \cite{Baraff,Brudigam2021a,Howell2022}, the ``kernel trick'' for state-vector machines \cite{Hofmann2006}, or the observation that solving large, sparse nonlinear optimization problems is often more effective than solving small, tightly-coupled dense problems.
% \todo{add citations from the trajectory optimization literature}.

\subsection{Extended Dynamic Mode Decomposition} \label{sec:edmd}

\todo{This section is too terse. A little more detail in the text about what's going on
would help, e.g. ``data matrices consisting of all control inputs and lifted states...''
Explain a little more in words and lean a little less on people parsing all of the math.}

A lifted bilinear system of the form \eqref{eq:bilinear_dynamics} can be learned from $P$
samples of the system dynamics $(x_j^+,x_j,u_j)$ using Extended Dynamic Mode Decomposition
(EDMD) \cite{Williams2015,Folkestad2021}. We concatenate all of the model coefficient
matrices as follows:
\begin{equation} \label{eq:E_matrixdef}
  E = \begin{bmatrix} A & B & C_1 & \dots & C_m \end{bmatrix} \in \R^{N_y \times N_z},
\end{equation}
by solving the following linear least-squares problem:
\todo{typo on $Y_{1:P}^2$ below?}
\begin{align} \label{opt:edmd}
  \underset{E}{\text{minimize}} \; \norm{E Z_{1:P} - Y_{1:P}^+}_2^2
\end{align}
where $Z_{1:P} \in \R^{N_z \times P}$ and $Y_{1:P}^+ \in \R^{N_y \times P}$ are the data 
matrices 
\begin{equation}
  Z_{1:P} = \begin{bmatrix}
    y_1         & y_2         & \dots  & y_P          \\
    u_1         & u_2         & \dots  & u_P          \\
    u_{1,1} y_1 & u_{2,1} y_2 & \dots  & u_{P,1} y_P  \\
    \vdots      & \vdots      & \ddots & \vdots       \\
    u_{1,m} y_1 & u_{2,m} y_2 & \dots  & u_{P,m} y_P  \\
  \end{bmatrix}, \quad 
  Y_{1:P}^+ = \begin{bmatrix}
    y_1^+         & y_2^+         & \dots  & y_P^+    \\
  \end{bmatrix},
\end{equation}
and $N_z = N_y + N_u + N_y \cdot N_u$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Methodology
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Jacobian-Regularizated Dynamic Mode Decomposition} \label{sec:jdmd}

\todo{Let's standardize on ``Jacobian-Regularized'' instead of ``Jacobian-penalized'' and
``JDMD'' instead of jDMD}

We now present JDMD as a straightforward adaptation of the original EDMD algorithm described
in Section \ref{sec:edmd}. Given $P$ samples of the dynamics $(x_i^+, x_i, u_i)$, and an
approximate discrete-time dynamics model,
\begin{equation}
  x^+ = \tilde{f}(x,u),
\end{equation}
we can evaluate the Jacobians of our approximate model $\tilde{f}$ at each of the sample
points: $\tilde{A}_i = \pdv{\tilde{f}}{x}, \tilde{B}_i = \pdv{\tilde{f}}{u}$. After
choosing a nonlinear mapping $\phi : \R^{N_x} \mapsto \R^{N_y}$ our goal is to find a
bilinear dynamics model \eqref{eq:bilinear_dynamics} that matches the Jacobians of our
approximate model, while also matching our dynamics samples. We accomplish this by 
penalizing differences between the Jacobians of our learned bilinear model with respect to 
the original states $x$ and controls $u$, and the Jacobians we expect from our analytical 
model. These \textit{projected Jacobians} are calculated by differentiating through the 
\textit{projected dynamics}:
\begin{equation} \label{eq:projected_dynamics}
  x^+ = G \left( A \phi(x) + B u + \sum_{i=1}^m u_i C_i \phi(x) \right)  = \bar{f}(x,u).
\end{equation}
Differentiating \eqref{eq:projected_dynamics} with respect to $x$ and $u$ gives us
\todo{This notation isn't super clear. Can we try using some different letters instead of
hats on $A$ and $B$ below? Also is there a typo on the $\bar{A}$ below?}
\begin{subequations} \label{eq:projected_jacobians}
  \begin{align}
    \bar{A}_j &= \pdv{\hat{f}}{x}\left(x_j,u_j\right) 
    = G \left(A + \sum_{i=1}^m u_{j,i} C_i \right) \Phi(x_j)
    %   = G A_j^x \phi(x_j)
    = G E \hat{A}(x_j,u_j) = G E \hat{A}_j \\
    \bar{B}_j &= \pdv{\hat{f}}{u}\left(x_j,u_j\right) 
    = G \Big(B + \begin{bmatrix} C_1 x_j & \dots & C_m x_j \end{bmatrix} \Big)
    %   = G B_j^u
    = G E \hat{B}(x_j,u_j) = G E \hat{B}_j
  \end{align}
\end{subequations}
where $\Phi(x) = \pdv*{\phi}{x}$ is the Jacobian of the nonlinear map $\phi$, and
\begin{equation}
  \hat{A}(x,u) =  \begin{bmatrix} 
    I_{N_y} \\ 0 \\ u_1 I_{N_y} \\ u_2 I_{N_y} \\ \vdots \\ u_m I_{N_y} 
  \end{bmatrix} \Phi(x) \in \R^{N_z \times N_x}, \quad
  \hat{B}(x,u) = \begin{bmatrix} 
    0 \\ 
    I_{N_u} \\ 
    [x \; 0 \; ... \; 0] \\
    [0 \; x \; ... \; 0] \\
    \vdots \\
    [0 \; 0 \; ... \; x] \\
  \end{bmatrix} \in \R^{N_z \times N_u}.
\end{equation}

We then solve the following linear least-squares problem:
\begin{align} \label{opt:jdmd}
  \underset{E}{\text{minimize}} \;\; 
    (1-\alpha) \norm{E Z_{1:P} - Y_{1:P}^+}_2^2 + 
        \alpha \sum_{j=1}^P \left( 
          \norm{G E \hat{A}_j - \tilde{A}_j}_2^2 + 
          \norm{G E \hat{B}_j - \tilde{B}_j}_2^2 \right)
\end{align}

The resulting linear least-squares problem has $(N_y + N_x^2 + N_x \cdot N_u) \cdot P$ rows
and $N_y \cdot N_z$ columns. Given that the number of rows in this problem grows
quadratically with the state dimension, solving this problem can be challenging from a
computational perspective. In the Section \ref{sec:rls}, we propose an algorithm for solving
these problems without needing to move to a distributed-memory setup in order to solve these
large linear systems. The proposed method also provides a straightforward way to approach
incremental updates to the bilinear system, where the coefficients could be efficiently
learned ``live'' while the robot gathers data by moving through its environment.

% If we define $\hat{A}_j \in
% \R^{N_x \times N_x}$ and $\hat{B}_j \in \R^{N_x \times N_u}$ to be the Jacobians of our
% bilinear dynamics model, projected back into the original state space (a formal definition
% of these terms will be provided in a few paragraphs), our objective is to find the
% matrices that parameterize our bilinear dynamics model, $A \in \R^{N_y \times N_y},B \in
% \R^{N_y \times N_u}$, and $C_{1:m} \in \R^{N_u} \times \R^{N_y \times N_y}$, that minimize
% the following objective:

% \begin{equation} \label{eq:lls_objective}
%   (1- \alpha) \sum_{j=1}^P \norm{\hat{y}_j - y_j^+}_2^2 + 
%   \alpha  \sum_{j=1}^P \norm{\hat{A}_j - \tilde{A}_j}_2^2 + 
%   \norm{\hat{B}_j - \tilde{B}_j}_2^2 
% \end{equation}
% where $\hat{y}_j^+ = g\left(\phi(x_j), u_j\right)$ is the output of our bilinear dynamics
% model, and $y_j^+ = \phi(x_j^+)$ is the actual lifted state (i.e. observables) at the next
% time step. Note that $\hat{y}_j$, $\hat{A}_j$, and $\hat{B}_j$ are all implicitly
% functions of the model parameters $A$, $B$, and $C_{1:m}$ we're trying to learn.

% While not immediately apparent, we can minimize \eqref{eq:lls_objective} using linear
% least-squares, using techniques similar to those used previously in the literature
% \cite{Folkestad2021}.

% To start, we combine all the data we're trying to learn into a single matrix:
% \begin{equation}
%   E = \begin{bmatrix} A & B & C_1 & \dots & C_m \end{bmatrix} \in \R^{N_y \times N_z},
% \end{equation}
% where $N_z = N_y + N_u + N_y \cdot N_u$.  We now rewrite the terms in
% \eqref{eq:lls_objective} in terms of $E$. By defining the vector 
% \begin{equation}
%   z = \begin{bmatrix} y^T & u^T & u_1 y^T & \dots & u_m y^T \end{bmatrix} \in \R^{N_z},
% \end{equation}
% we can write down 
% the output of our bilinear dynamics \eqref{eq:bilinear_dynamics} as 
% \begin{equation} \label{eq:bilinear_dynamics_z}
%   \hat{y}^+ = E z.
% \end{equation}
% The previously-mentioned projected Jacobians of our bilinear model, $\hat{A}$ and
% $\hat{B}$, are simply the Jacobians of the bilinear dynamics in terms of the original
% state. We obtain these dynamics by ``lifting`` the state via $\phi$ and then projecting
% back onto the original states using $G$:
% \begin{equation} \label{eq:projected_dynamics}
%   x^+ = G \left( A \phi(x) + B u + \sum_{i=1}^m u_i C_i \phi(x) \right)  = \hat{f}(x,u) 
% \end{equation}
% Differentiating these dynamics gives us our projected Jacobians:
% \begin{subequations} \label{eq:projected_jacobians}
%   \begin{align}
%     \hat{A}_j &= \pdv{\hat{f}}{x}\left(x_j,u_j\right) 
%     = G \left(A + \sum_{i=1}^m u_{j,i} C_i \right) \Phi(x_j)
%     %   = G A_j^x \phi(x_j)
%     = G E \bar{A}(x_j,u_j) = G E \bar{A}_j \\
%     \hat{B}_j &= \pdv{\hat{f}}{u}\left(x_j,u_j\right) 
%     = G \Big(B + \begin{bmatrix} C_1 x_j & \dots & C_m x_j \end{bmatrix} \Big)
%     %   = G B_j^u
%     = G E \bar{B}(x_j,u_j) = G E \bar{B}_j
%   \end{align}
% \end{subequations}
% where $\Phi(x) = \pdv*{\phi}{x}$ is the Jacobian of the nonlinear map $\phi$, and
% \begin{equation}
%   \bar{A}(x,u) =  \begin{bmatrix} 
%     I \\ 0 \\ u_1 I \\ u_2 I \\ \vdots \\ u_m I 
%   \end{bmatrix} \in \R^{N_z \times N_x}, \quad
%   \bar{B}(x,u) = \begin{bmatrix} 
%     0 \\ 
%     I \\ 
%     [x \; 0 \; ... \; 0] \\
%     [0 \; x \; ... \; 0] \\
%     \vdots \\
%     [0 \; 0 \; ... \; x] \\
%   \end{bmatrix} \in \R^{N_z \times N_u}.
% \end{equation}
% Note we define $\bar{A}_j = \bar{A}(x_j,u_j)$, $\bar{B}_j = \bar{B}(x_j,u_j)$ to lighten 
% the notation, but want to emphasize that these terms are all purely functions of the input
% data.

% Substituting \eqref{eq:bilinear_dynamics_z} and \eqref{eq:projected_jacobians} into
% \eqref{eq:lls_objective}, we can rewrite our LLS problem as:
% \begin{align}
%   \underset{E}{\text{minimize}} \;\; 
%   \sum_{j=0}^P
%   (1-\alpha) \norm{E z_j - y_j^+}_2^2 + 
%   \alpha  \norm{G E \bar{A}_j - \tilde{A}_j}_2^2 + 
%   \alpha  \norm{G E \bar{B}_j - \tilde{B}_j}_2^2 
% \end{align}
% which is equivalent to
% \begin{align} \label{opt:lls_matrices}
%   \underset{E}{\text{minimize}} \;\; 
%   (1-\alpha) \norm{E \mathbf{Z_{1:P}} - \mathbf{Y^+_{1:P}} }_2^2 + 
%   \alpha  \norm{G E \mathbf{\bar{A}_{1:P}} - \mathbf{\tilde{A}_{1:P}}}_2^2 + 
%   \alpha  \norm{G E \mathbf{\bar{B}_{1:P}} - \mathbf{\tilde{B}_{1:P}}}_2^2
% \end{align}
% where $\mathbf{Z_{1:P}} \in \R^{N_z \times P} = [z_1 \; z_2 \; ... \; z_P]$ horizontally
% concatenates all of the samples (equivalent definition for 
% $\mathbf{Y^+_{1:P}} \in \R^{N_y \times P}$, 
% $\mathbf{\bar{A}_{1:P}} \in \R^{N_z \times N_x \cdot P}$, 
% $\mathbf{\tilde{A}_{1:P}} \in \R^{N_z \times N_x \cdot P}$,
% $\mathbf{\bar{B}_{1:P}} \in \R^{N_z \times N_u \cdot P}$, and 
% $\mathbf{\tilde{B}_{1:P}} \in \R^{N_z \times N_u \cdot P}$ ).

% We can rewrite \eqref{opt:lls_matrices} in standard form using the ``vec trick''
% \begin{equation} \label{eq:vectrick}
%   \text{vec}(A X B) = (B^T \otimes A) \text{vec}(X)
% \end{equation}
% where $\text{vec}(A)$ stacks the columns of $A$ into a single vector.

% Setting $E$ in \eqref{opt:lls_matrix} equal to $X$ in \eqref{eq:vectrick}, we get
% \begin{align} \label{opt:lls_matrix}
%   \underset{E}{\text{minimize}} \;\;  
%   \norm{
%   \begin{bmatrix}
%     (1-\alpha)\cdot(\mathbf{Z_{1:P}})^T \otimes I_{N_y} \\
%     \alpha\cdot(\mathbf{\bar{A}_{1:P}})^T \otimes G \\
%     \alpha\cdot(\mathbf{\bar{G}_{1:P}})^T \otimes G \\
%   \end{bmatrix}
%   \text{vec}(E)
%   +
%   \begin{bmatrix}
%     (1-\alpha)\cdot\text{vec}(\mathbf{Y^+_{1:P}}) \\
%     \alpha\cdot\text{vec}(\mathbf{\tilde{A}_{1:P}}) \\
%     \alpha\cdot\text{vec}(\mathbf{\tilde{G}_{1:P}})
%   \end{bmatrix}
%   }_2^2
% \end{align}
% such that the matrix of coefficients has $(N_y + N_x^2 + N_x \cdot N_u) \cdot P$ rows and 
% $N_y \cdot N_z$ columns. We obtain the data for our bilinear model 
% \eqref{eq:bilinear_dynamics} by solving this large, sparse linear least-squares 
% problem.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Efficient Recursive Least Squares} \label{sec:rls}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In its canonical formulation, a linear least squares problem can be represented as the
following unconstrained optimization problem:
\begin{align} \label{opt:lls}
  \min_x \|Fx - d\|_2^2.
\end{align}
We assume $F$ is a large, sparse matrix and that solving it directly using a QR or Cholesky
decomposition requires too much memory for a single computer. While solving \eqref{opt:lls}
using an iterative method such as LSMR \cite{Fong2011} or LSQR \cite{Paige1982} is possible,
we find that these methods do not work well in practice for solving \eqref{opt:jdmd} due to
ill-conditioning.  Standard recursive methods for solving these problems are able to process
the rows of the matrices sequentially to build a QR decomposition of the full matrix, but
also tend to suffer from ill-conditioning \cite{Strobach1990,Sayed2009,Ghirnikar1990}.

To overcome these issues, we propose an alternative recursive method based. We solve
\eqref{opt:lls} by dividing up rows of $F$ into batches:
\begin{align} \label{eq:F_sum}
  F^T F = F_1^T F_1 + F_2^T F_2 + \ldots + F_N^T F_N.
\end{align}
The main idea is to maintain and update an upper-triangular Cholesky factor $U_i$ of the
first $i$ terms of the sum \eqref{eq:F_sum}. Given $U_i$, we can calculate $U_{i+1}$ using
the $\operatorname{QR}$ decomposition, as shown in
\cite{Howell2019}:
\begin{equation}
  U_{i+1} = \sqrt{U_i^TU_i + F_{i+1}^TF_{i+1}} = 
  \operatorname{QR_R}\bigg( \begin{bmatrix} {U_i} \\ {F_{i+1}} \end{bmatrix} \bigg),
\end{equation}
where $\operatorname{QR_R}$ returns the upper triangular matrix $R$ from the 
$\operatorname{QR}$ decomposition. For an efficient implementation, this function should be
an ``economy'' or ``Q-less'' $\operatorname{QR}$ decomposition 
% \cite{cite something (e.g.  Van Loan)}, 
since the $Q$ matrix is never needed.

We also handle regularization of the normal equations, equivalent to adding Tikhonov
regularization to the original least squares problem \todo{cite something}, during the base
case of our recursion. If we want to add an L2 regularization with weight $\lambda$, we
calculate $U_1$ as:

\begin{equation}
  U_1 =  \operatorname{QR_R}\bigg( 
  \begin{bmatrix} {F_1} \\ \sqrt{\lambda} I \end{bmatrix}.
  \bigg).
\end{equation}

% The final algorithm for solving a least squares problem in a recursive-batch fashion is
% described in algorithm \ref{alg:rlsqr}. This algorithm can be modified to handle L2
% regularized least squares problems by simply replacing line 2 of algorithm \ref{alg:rlsqr}
% with $U\leftarrow \operatorname{QR_R}(\operatorname{vcat}(F_1,\sqrt{\rho}I))$, where $\rho$
% is the regularizer. 
% \begin{algorithm} 
%   \begin{algorithmic}[1]
%     \caption{Recursive Batch Least Squares with QR}\label{alg:rlsqr}
%     \State \textbf{input} $F,\,d$  \Comment{problem data}
%     \State $U \leftarrow \operatorname{QR_R}(\operatorname{vcat}(F_1,\sqrt{\rho}I))$ \Comment{form initial upper-triangular square-root}
%     \State $b \leftarrow F_1^Td_1$ \Comment{form initial right hand side vector}
%     \For{$i = 2:N$}
%     % 		\State $U \leftarrow \operatorname{QR_R}([U^T \, A_i^T]^T) $ 
%     \State $U \leftarrow \operatorname{QR_R}(\operatorname{vcat}(U,F_i)) $ \Comment{update square-root with new batch}
%     \State $b \leftarrow b + F_i^Td_i$ \Comment{update right hand side with new batch}
%     \EndFor
%     \State \textbf{output} \,$x \leftarrow U^{-1}U^{-T}b$ \Comment{forward and backwards substitution to solve for $x$}
%   \end{algorithmic}
% \end{algorithm}
\begin{figure}
  \centering
  \includegraphics[width=\textwidth]{perch_cropped.png}
  \caption{Airplane perching trajectory, a high angle-of-attack maneuver that minimizes 
    velocity at the goal position}
  \label{fig:perch}
\end{figure}

\section{Projected Bilinear MPC} \label{sec:projected_mpc}

We propose a simple approach to model-predictive control for the bilinear systems learned
using either classic EDMD or the proposed JDMD approach. The key idea is to use the 
projected Jacobians $\bar{A}$ and $\bar{B}$ in \eqref{eq:projected_jacobians}, effectively
reducing the problem to a standard linear MPC problem in the original state space instead of
the larger, lifted one.  In the all of the examples in the following section, our MPC
controller solves the following convex Quadratic Program (QP):
% \begin{mini}[2]
%   {x_{1:N}, u_{1:N}}{\half x_N^T Q_N x_N + \frac \sum_{k=1}^{N-1} x_k^T Q_k x_k + u_k^T R_k u_k}{}{}
% \end{mini}
\begin{mini}
  {x_{1:N}, u_{1:N-1}}{\half x_N^T Q_N x_N + \half \sum_{k=1}^{N-1} x_k^T Q_k x_k + u_k^T R_k u_k }{}{}
  \addConstraint{x_{k+1} = \bar{A}_k x_k + \bar{B}_k u_k + d_k}{}
  \addConstraint{x_1 = x_\text{init}}
\end{mini}
where here we define $x$ and $u$ to be the ``delta''  from the reference trajectory
\todo{how about just writing $\Delta x$ and $\Delta u$ explicitly above?} $\bar{x}_{1:N}$,
$\bar{u}_{1:N-1}$. The affine dynamics term $d_k = f(\bar{x}_k, \bar{u}_k) - \bar{x}_{k+1}$
allows for dynamically infeasible reference trajectories. The projected Jacobians can be
efficiently calculated from the bilinear dynamics either offline or online, and since the
problem dimension is the same size as the linear MPC problem for the original dynamics, it
is no more expensive to compute. This formulation also makes it trivial to enforce
additional control or path constraints, and 
avoids the need to regularize or otherwise constrain the lifted states.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Results
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results} \label{sec:results}

This section presents the results of several simulation experiments to evaluate the
performance of JDMD.  For each simulated system we specify two models: a \textit{nominal}
model, which is simplified and contains both parametric and non-parametric model error, and
a \textit{true} model, which is used exclusively for simulating the system and evaluating
algorithm performance.

All models were trained by simulating the ``true'' system with a nominal controller to 
collect data in the region of the state space relevant to the task. A set of fixed-length 
trajectories were collected, each at a sample rate of 20-25 Hz. The bilinear EDMD model was
trained using the same approach introduced by Folkestad and Burdick \cite{Folkestad2021}.
All continuous dynamics were discretized with an explicit fourth-order Runge Kutta
integrator. Code for all experiments is available at \todo{removed for anonymous review}.  

\subsection{Systems and Tasks}

\textbf{Cartpole:} We perform a swing-up task on a cartpole system. The \textit{true} model
includes Coulomb friction between the cart and the floor, viscous damping at both joints,
and a deadband in the control input that were not included in the \textit{nominal} model.
Additionally, the mass of the cart and pole model were altered by 20\% and 25\% with respect
to the nominal model, respectively.  The following nonlinear mapping was used when learning
the bilinear models: 
$\phi(x) = [\, 1,\,
x,\, \sin(x),\, \cos(x),\, \sin(2x),\, \sin(4x),\, T_2(x),\, T_3(x),\, T_4(x)\, ] \in
\R^{33}$, where $T_i(x)$ is a Chebyshev polynomial of the first kind of order $i$. 
All reference trajectories for the swing up task were generated using ALTRO 
\cite{Howell2019,Jackson2021}.

\textbf{Quadrotor:} We attempt to track point-to-point linear reference trajectories from
various initial conditions on both planar and full 3D quadrotor models. For both systems,
the \textit{true} model includes aerodynamic drag terms not included in the \textit{nominal}
model, as well as parametric error of roughly 5\% on the system parameters (e.g. mass, rotor
arm length, etc.). The planar model was trained using a nonlinear mapping of $\phi(x) = [\,
1,\, x,\, \sin(x),\, \cos(x),\, \sin(2x),\, T_2(x) ] \in \R^{25}$ while the full quadrotor
model was train using a nonlinear mapping of $\phi(x) = [\, 1,\, x,\, T_2(x),\, \sin(p),\,
\cos(p),\, R^{T}v,\ v^{T}RR^{T}v,\, p \times v,\, p \times \omega,\, \omega \times \omega ]
\in \R^{44}$, where $p$ is the quadrotor's position, $v$ and $\omega$ are the translational
and angular velocities respectively, and $R$ is the rotation matrix.

\textbf{Airplane:} We perform a post-stall perching maneuver on a high-fidelity model of an
airplane constructed from wind-tunnel data \cite{Manchester2017}. A demonstration perching
trajectory was produced using trajectory optimization (see Figure \ref{fig:perch}) and then
tracked using MPC. The nominal model uses a simple flat-plate wing model with linear lift
and quadratic drag coefficient approximations. The bilinear models use a 68-dimensional
nonlinear mapping $\phi$ including terms such as the rotation matrix (expressed in terms of
a Modified Rodriguez Parameter), powers of the angle of attack and side slip angle, the body
frame velocity, various cross products with the angular velocity, and some 3rd and 4th order
Chebyshev polynomials \todo{of what?}.

% nonlinear mapping: $\phi(x) = [1, x, \text{vec}(R), v_\text{body}, v_\text{body}^T
% v_\text{body}, \sin(p), \alpha, \beta, \alpha^2, \beta^2, \alpha^3, \beta^3, p \times v, p
% \times \omega, \omega \times \omega, T_3(x), T_4(x) ] \in \R^{68}$, where $p \in \R^3$ is
% the position, $g \in \R^3$ is the attitude expressed as Modified Rodriguez Parameter, $v \in
% \R^3$ is the linear velocity in the world frame, $\omega \in \R^3$ is the angular velocity,
% $R \in \R^{3 \times 3}$ is the rotation matrix expressed in terms of $g$, $v_\text{body}$ is
% the velocity in the body frame, $\alpha$ is the angle of attack, and $\beta$ is the side
% slip angle.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Sample Efficiency}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.48\textwidth}
    \includegraphics[width=\textwidth, height=5cm]{../images/cartpole_mpc_test_error.tikz}
    % \includegraphics[width=\textwidth, height=5cm]{combined_mpc_test_error.tikz}
    \caption{Cartpole}
    \label{fig:cartpole_mpc_test_error}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    % \includegraphics[width=\textwidth, height=5cm]{rex_planar_quadrotor_mpc_test_error.tikz}
    \includegraphics[width=\textwidth,height=5cm]{airplane_error_by_num_train.tikz}
    \caption{Airplane}
    % \label{fig:planar_quad_mpc_test_error}
    % \includegraphics[width=\textwidth, height=5cm]{cartpole_mpc_train_time.tikz}
    % \caption{Training time for cartpole models as a function of training samples, using a 
    % preliminary implementation of the algorithm described in Section \ref{sec:rls}. The 
    % training time complexity is approximately linear.}
    % \label{fig:cartpole_train_time}
  \end{subfigure}
  \caption{MPC tracking error vs training trajectories for both the cartpole (left) and
  airplane (right). Tracking error is defined as the average L2 error over all the
  test trajectories between the reference and simulated trajectories.}
  \label{fig:sample_efficiency}
\end{figure}

We highlight the sample efficiency of the proposed algorithm in Figure 
\ref{fig:sample_efficiency}. For both the cartpole swing up and the airplane perch trajectory 
tracking tasks, the proposed method achieves better tracking than the nominal MPC controller
with just two sample trajectories, and performs better than EDMD on both 
trajectory tracking tasks. To achieve comparable performance on the perching task, EDMD 
requires about 4x the number of samples (20 vs 5) compared to the proposed approach.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Generalization}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{figure}[t] \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_equilibrium_change.tikz}
    \caption{LQR stabilization error over increasing equilibrium offset}
    \label{fig:rex_planar_quadrotor_lqr_error_by_equilibrium_change}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.48\textwidth}
    \raggedright
    \includegraphics[width=\textwidth, height=5cm]{rex_planar_quadrotor_mpc_error_by_training_window.tikz}
    \caption{Tracking error for the quadrotor MPC reference trajectory tracking task.}
    \label{fig:rex_planar_quadrotor_mpc_error_by_training_window}
    % \includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_training_window.tikz}
    % \caption{Stabilization error for the quadrotor LQR stabilization task}
    % \label{fig:rex_planar_quadrotor_lqr_error_by_training_window}
  \end{subfigure}
  \caption{Generalizability with respect to initial conditions sampled outside of the 
  training domain. The initial conditions are sampled from a uniform distribution, whose 
  limits are determined by a scaling of the limits used for the training distribution. 
  A training range fraction greater than $1$ indicates the
  distribution range is beyond that used to generate the training trajectories. The thick 
  lines represent the algorithm with a heavy regularization parameter.
  }
  \label{fig:training_window}
\end{figure}

We demonstrate the generalizability of the proposed method on both the planar and 3D 
quadrotor. In all tasks, the goal is to return to the origin, given an initial condition 
sampled from some uniform distribution centered at the origin. To test the generalizability
of the algorithms, we scale the size of the sampling ``window'' relative to the window on 
which it was trained, 
e.g. if the initial lateral position was trained on 
data in the interval $[-1.5,+1.5]$, we sampled the test initial condition from the window 
$[-\gamma 1.5, +\gamma 1.5]$. The results for the planar quadrotor are shown in Figure 
\ref{fig:rex_planar_quadrotor_mpc_error_by_training_window}. With a well-picked 
regularization value, JDMD generalized past where the performance of EDMD suffers. 
Additionally, in Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_equilibrium_change} we 
show the effect of changing the equilibrium position away from the origin: while the true 
dynamics should be invariant to this change, EDMD fails to learn this whereas JDMD does.

\begin{wraptable}{r}{7.0cm}
	\vspace{-2\baselineskip}
	\begin{tabular}{cccc}\\
		\toprule  
								& {\color{black} \textbf{Nominal}} & {\color{orange} \textbf{EDMD}} & {\textbf{\color{cyan} JDMD}} \\
		\midrule
		Tracking Err.		& 0.30			& 0.63 	& \textbf{0.11} \\
		Success Rate 			& \textbf{82\%} & 18\%	& 80\% \\
		\bottomrule
	\end{tabular}
	\caption{Performance summary of MPC tracking of 6-DOF quadrotor}
	\vspace{-1\baselineskip}
	\label{tab:full_quad_tracking_mpc}
\end{wraptable} 
For the full quadrotor, given the goal of tracking a straight line back to the origin, we
test 50 initial conditions, many of which are far from the goal, have large velocities, or
are nearly inverted (see Figure \ref{fig:rex_full_quadrotor_initial_conditions}). 
The results using an MPC controller are shown in Table \ref{tab:full_quad_tracking_mpc}, 
demonstrating the excellent generalizability of the algorithm, given that the algorithm 
was only trained on 30 initial conditions, sampled relatively sparsely given the size of the 
sampling window. EDMD only successfully brings about 18\% of the samples to the origin, 
while the majority of the time resulting in trajectories like those in Figure 
\ref{fig:jdmd_full_quad_pointtopoint_with_waypoints}.


% We demonstrate the generalizability of the proposed method to tasks outside of its training
% domain in Figure \ref{fig:training_window}. In both the planar quadrotor 
% stabilization (Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_equilibrium_change}) and 
% trajectory tracking (Figure \ref{fig:rex_planar_quadrotor_mpc_error_by_training_window})
% tasks, we trained the models by sampling uniformly from a given window of offsets, centered 
% about the origin. 
% To test the generalizability of the methods we increased the relative size of the window 
% from which the test data was sampled, e.g. if the initial lateral position was trained on 
% data in the interval $[-1.5,+1.5]$, we sampled the test initial condition from the window 
% $[-\gamma 1.5, +\gamma 1.5]$. As shown in the results, while the performance of the proposed
% algorithm remains relatively constant even when $\gamma = 2.5$, whereas the classic EDMD 
% approach looses performance and fails to generalize at all for the stabilization task using 
% and LQR controller (like due to poor derivative information), and up to $\gamma = 2$ for the 
% tracking task using a linear MPC controller.

% In Figure \ref{fig:rex_planar_quadrotor_lqr_error_by_equilibrium_change} we show the effect 
% of changing the equilibrium position for the planar quadrotor, but keeping the delta initial
% conditions within the training window. As shown, EDMD doesn't generalize to other
% equilibrium points, despite the fact that the underlying dynamics are invariant to the
% equilibrium position. Our proposed approach, however, easily learns this from the derivative
% information provided by the nominal model.

% To demonstrate that this generalizability extends to more complex dynamics, we additionally 
% show the tracking capabilities of infeasible, point-to-point trajectories for the 6-DOF quadrotor
% with full attitude dynamics. As seen in Figure \ref{fig:rex_full_quadrotor_initial_conditions}, the generated
% reference trajectories have a wide scope in sampling despite being sparse in nature. As shown in
% Table \ref{tab:full_quad_tracking_mpc}, JDMD has the best tracking performance and closely matches nominal MPC's ability to
% successfully reach the equilibrium. Meanwhile, EDMD has significantly worse performance in tracking and succeeding
% in stabilizing about the goal state.  Despite the aggressive starting state, JDMD is still able to successfully
% account for the attitude dynamics, reach the goal state, and stabilize, as seen in Figure \ref{fig:jdmd_full_quad_pointtopoint_with_waypoints}.

\begin{figure}[t] \centering
	\begin{subfigure}[t]{0.49\textwidth}
		\centering
		\includegraphics[width=\textwidth,height=5cm]{full_quadrotor_test_linear_trajectories.png}
		\caption{Generated point-to-point trajectories and initial conditions for testing
		tracking MPC of 6-DOF quadrotor.}
		\label{fig:rex_full_quadrotor_initial_conditions}
	\end{subfigure}
	\hfill
	\begin{subfigure}[t]{0.49\textwidth}
		\raggedright
		\includegraphics[width=\textwidth, height=5cm]{jdmd_full_quad_pointtopoint_with_waypoints.png}
		\caption{Generated MPC trajectories for nominal MPC (black), EDMD (\color{orange}
		orange\color{black}), and JDMD (\color{cyan} cyan\color{black}) for tracking infeasible,
		point-to-point trajectory (\color{red} red\color{black}).}
		\label{fig:jdmd_full_quad_pointtopoint_with_waypoints}
		% \includegraphics[width=\textwidth,height=5cm]{rex_planar_quadrotor_lqr_error_by_training_window.tikz}
		% \label{fig:rex_planar_quadrotor_lqr_error_by_training_window}
	\end{subfigure}
	\caption{Generalizability with respect to initial conditions sampled outside of the 
		training domain. The initial conditions are sampled from a uniform distribution, whose 
		limits are determined by a scaling of the limits used for the training distribution. 
		A training range fraction greater than $1$ indicates the
		distribution range is beyond that used to generate the training trajectories. The thick 
		lines represent the algorithm with a heavy regularization parameter.
	}
	\label{fig:training_window}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lifted versus Projected MPC}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{wraptable}{r}{5.0cm}
  \vspace{-3.5\baselineskip}
  \begin{tabular}{ccc}\\
    \toprule  
    MPC       & {\color{orange} \textbf{EDMD}} & {\textbf{\color{cyan} JDMD}} \\
    \midrule
    Lifted    & 17   &          15 \\
    Projected & 18   &  \textbf{2} \\
    \bottomrule
  \end{tabular}
  \caption{Training trajectories required to beat nominal MPC}
  \vspace{-1\baselineskip}
  \label{tab:mpc_comp}
\end{wraptable} 

We performed a simple experiment to highlight the value of the proposed ``projected'' MPC,
outlined in Section \ref{sec:projected_mpc}. We trained EDMD and JDMD models with an
increasing number of training trajectories, and recorded the first sample size at which the
``lifted'' and ``projected'' MPC controllers consistently stabilized the system (i.e.
stabilized 95\% of the test initial conditions for the cartpole system for that sample size
and subsequent ones).  The results are summarized in Table \ref{tab:mpc_comp}. The results
quantitatively show what we qualitatively observed while training and testing these various
examples: the projected MPC approach usually required far fewer samples to ``train'' and
usually had better performance than its lifted counterpart that used the bilinear
lifted dynamics. This was especially pronounced when combined with the proposed JDMD
approach, which makes sense given that the approach explicitly encourages these Jacobians to
match the analytical ones, so quickly converges to reasonable values with just a few training
examples.

\subsection{Sensitivity to Model Mismatch}

While we've introduced a significant mount of model mismatch in all of the examples so far,
a natural argument against model-based methods is that they're only as good as your model is
at capturing the salient dynamics of the system.  We investigated the effect of increasing
model mismatch by incrementally increasing the Coulomb friction coefficient between the cart
and the floor for the cartpole stabilization task (recall the nominal model assumed zero
friction). The results are shown in Table \ref{tab:friction_comp}. As expected, the number
of training trajectories required to find a good stabilizing controller increases for the
proposed approach. We achieved the results above by setting $\alpha = 0.01$, corresponding 
to a decreased confidence in our model, thereby placing greater weight on the experimental 
data. The standard EDMD approach always required more samples, and was unable to find a good
enough model above friction values of 0.4. While this could likely be remedied by adjusting
the nonlinear mapping $\phi$, the proposed approach works well with the given bases.  Note
that the nominal MPC controller failed to stabilize the system above friction values of 0.1,
so again, we demonstrate that we can improve MPC performance substantially with just a few
training samples by combining analytical gradient information and data sampled from the true
dynamics.

\begin{table}[t]
  \centering
  \begin{tabular}{cccccccc}
  \toprule 
  Friction ($\mu$) & 0.0 & 0.1 & 0.2 & 0.3 & 0.4 & 0.5 & 0.6 \\
  \midrule 
  Nominal & \cmark & \cmark & \xmark & \xmark & \xmark & \xmark & \xmark \\
  EDMD & 3 & 19 & 6 & 14 & \xmark & \xmark & \xmark \\
  JDMD & 2 & 2 & 2 & 2 & 3 & 7 & 12 \\
  \end{tabular}
  % \include(../images/tables/friction_comp.tex)
  \caption{Training trajectories required to stabilize the cartpole with the given friction
    coefficient
  }
  \label{tab:friction_comp}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Limitations 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations} \label{sec:limitations}

As with most data-driven techniques, it is difficult to claim that our
method will increase performance in all cases. It is possible that having an extremely poor
prior model may hurt rather than help the training process. However, we found that even
when the $\alpha$ parameter is extremely small (placing little weight on the Jacobians 
during the learning process), it still dramatically improves the sample efficiency over 
standard EDMD. It is also quite possible that the performance gaps between EDMD and JDMD
shown here can be reduced through better selection of basis functions and better training
data sets; however, given that the proposed approach converges to EDMD as $\alpha
\rightarrow 0$, we see no reason to not adopt the proposed methodology as simply tune
$\alpha$ based on the confidence of the model and the quantity (and quality) of training
data.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Conclusion 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions and Future Work} \label{sec:conclusion}

We have presented JDMD, a simple but powerful extension to EDMD that incorporates derivative
information from an approximate prior model. We have tested JDMD in combination with 
a simple linear MPC control policy across a range of systems and tasks, and have found that the
resulting combination can dramatically increase sample efficiency over EDMD, often improving over a
nominal MPC policy with just a few sample trajectories. Substantial areas for future work
remain: most notably, demonstrating the proposed pipeline on hardware. Additional directions
include lifelong learning or adaptive control applications, combining simulated and real
data through the use of modern differentiable physics engines \cite{Howell2022,Todorov2012},
residual dynamics learning, as well as the development of specialized numerical methods for
solving nonlinear optimal control problems using the learned bilinear dynamics.

\bibliography{Koopman.bib}

\end{document}
